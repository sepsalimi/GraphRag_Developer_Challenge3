{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sepeh/.cache/pypoetry/virtualenvs/graphrag-developer-challenge3-4suqu0LP-py3.12/bin/python\n",
      "Kernel working. Proceed\n"
     ]
    }
   ],
   "source": [
    "# Confirm Jupyter and kernel works\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"Kernel working. Proceed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3/Main Functions\n",
      "Parent directory: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3\n",
      "Parent dir exists: True\n",
      "KnowledgeGraph exists: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Neo4j!\n"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j\n",
    "import os\n",
    "\n",
    "# Add parent directory to Python path so we can import KnowledgeGraph\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "print(f\"Parent dir exists: {os.path.exists(parent_dir)}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "print(f\"KnowledgeGraph exists: {os.path.exists(os.path.join(parent_dir, 'KnowledgeGraph'))}\")\n",
    "\n",
    "from KnowledgeGraph.knowledgegraph import ingest_Chunks, create_nodes, create_relationship, create_vector_index, embed_text\n",
    "from KnowledgeGraph.chunking import split_data_from_file\n",
    "from KnowledgeGraph.config import load_neo4j_graph\n",
    "\n",
    "import json\n",
    "\n",
    "# Error handling for Neo4j connection\n",
    "try:\n",
    "    graph, openAI_api, openAI_endpoint, openAI_model = load_neo4j_graph()\n",
    "    print(\"Successfully connected to Neo4j!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set up Neo4j (local or AuraDB) and update .env file\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting old Chunk, Document, Publication, Gazette, Issue, Issues, Page, Section nodes...\n",
      "Old nodes deleted successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Cleanup (optional reset between runs)\n",
    "# -------------------------------------------------------------\n",
    "labels_to_wipe = ['Chunk', 'Document', 'Publication', 'Gazette', 'Issue', 'Issues', 'Page', 'Section']\n",
    "print(f\"Deleting old {', '.join(labels_to_wipe)} nodes...\")\n",
    "try:\n",
    "    graph.query(\"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE n:Chunk OR n:Document OR n:Publication OR n:Gazette OR n:Issue OR n:Issues OR n:Page OR n:Section\n",
    "    DETACH DELETE n\n",
    "    \"\"\")\n",
    "    print(\"Old nodes deleted successfully.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Cleanup skipped: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <r> Clear Embeddings </r> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped index `Chunk` (if it existed).\n",
      "Cleared old embeddings from Chunk nodes.\n",
      "Recreated vector index `Chunk` with dims from OPENAI_EMBED_DIM.\n",
      "Index info: {'name': 'Chunk', 'options': {'indexProvider': 'vector-2.0', 'indexConfig': {'vector.hnsw.m': 16, 'vector.hnsw.ef_construction': 100, 'vector.dimensions': 3072, 'vector.similarity_function': 'COSINE', 'vector.quantization.enabled': True}}}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Recreate vector index at dims from .env and clear old vectors\n",
    "# -------------------------------------------------------------\n",
    "from textwrap import dedent\n",
    "\n",
    "def run(q, params=None):\n",
    "    return graph.query(q, params or {})\n",
    "\n",
    "# 1) Drop old index (idempotent)\n",
    "try:\n",
    "    run(\"DROP INDEX `Chunk` IF EXISTS\")\n",
    "    print(\"Dropped index `Chunk` (if it existed).\")\n",
    "except Exception as e:\n",
    "    print(f\"Drop index skipped: {e}\")\n",
    "\n",
    "# 2) Clear old vectors (they may be wrong dims)\n",
    "run(\"MATCH (c:Chunk) REMOVE c.textEmbeddingOpenAI\")\n",
    "print(\"Cleared old embeddings from Chunk nodes.\")\n",
    "\n",
    "# 3) Recreate index using dims from .env (OPENAI_EMBED_DIM)\n",
    "from KnowledgeGraph.knowledgegraph import create_vector_index\n",
    "create_vector_index(graph=graph, index_name='Chunk')  # reads OPENAI_EMBED_DIM\n",
    "print(\"Recreated vector index `Chunk` with dims from OPENAI_EMBED_DIM.\")\n",
    "\n",
    "# 4) Show index options to confirm dims\n",
    "rows = run(dedent(\"\"\"\n",
    "SHOW INDEXES\n",
    "YIELD name, type, entityType, labelsOrTypes, properties, options\n",
    "WHERE name = 'Chunk'\n",
    "RETURN name, options\n",
    "\"\"\"))\n",
    "print(\"Index info:\", rows[0] if rows else \"Index not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through each file and ingest main nodes and chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Project root: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3/Main Functions\n",
      "Source folder: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3/KnowledgeGraph/source_data\n",
      "Prepared 2025-04-27_en.md: 115 documents, 328 chunks\n",
      "Prepared 2025-05-01_en.md: 1 documents, 2 chunks\n",
      "Prepared 2025-05-04_en.md: 144 documents, 431 chunks\n",
      "Prepared 2025-05-11_en.md: 182 documents, 664 chunks\n",
      "Prepared 2025-05-15_en.md: 2 documents, 6 chunks\n",
      "Prepared 2025-05-20_en.md: 1 documents, 3 chunks\n",
      "Prepared 2025-05-25_en.md: 113 documents, 335 chunks\n",
      "Prepared 2025_04_06_en.md: 88 documents, 188 chunks\n",
      "Prepared 2025_04_13_en.md: 135 documents, 494 chunks\n",
      "Prepared 2025_04_20_en.md: 126 documents, 322 chunks\n",
      "Ingesting chunks (workers=16): 100%|███████████████████| 2773/2773 [00:09<00:00]\n",
      "Ingestion complete. Ingested 2773 chunks in 9.6s (289 chunks/s).\n"
     ]
    }
   ],
   "source": [
    "import json, re, uuid, shutil, time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "SOURCE_DIR = (PROJECT_ROOT.parent / \"KnowledgeGraph\" / \"source_data\").resolve()\n",
    "GAZETTE_NAME = \"Kuwait Today / Al-Kuwait Al-Youm\"\n",
    "max_workers = int(os.getenv(\"MAX_WORKERS\", str(min(16, (os.cpu_count() or 4) * 2))))\n",
    "\n",
    "if not SOURCE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Expected source markdown directory at {SOURCE_DIR}\")\n",
    "\n",
    "tqdm.write(f\"Project root: {PROJECT_ROOT}\", file=sys.stderr)\n",
    "tqdm.write(f\"Source folder: {SOURCE_DIR}\", file=sys.stderr)\n",
    "\n",
    "METADATA_RE = re.compile(r\"<document_metadata>\\s*(\\{.*?\\})\\s*</document_metadata>\", re.DOTALL)\n",
    "DATE_FROM_FILENAME_RE = re.compile(r\"(\\d{4})[-_](\\d{2})[-_](\\d{2})\")\n",
    "SUPPLEMENT_NUMBER_RE = re.compile(r\"Supplement\\s*(?:No\\.?|Number)?\\s*(\\d+)\", re.IGNORECASE)\n",
    "ISSUE_RE = re.compile(r\"Issue\\s*(?:No\\.?|Number)?\\s*(\\d{3,4})\", re.IGNORECASE)\n",
    "BLOCK_RE = re.compile(r\"<(document_metadata|page_metadata)>.*?</\\1>\", re.DOTALL)\n",
    "PAGE_START_RE = re.compile(r\"<page_start[^>]*>(\\d+)</page_start>\")\n",
    "PAGE_END_RE = re.compile(r\"<page_end[^>]*>(\\d+)</page_end>\")\n",
    "DOC_NUMBER_PATTERNS = [\n",
    "    re.compile(r\"No\\.?\\s*\\(?\\s*([A-Za-z0-9\\s\\-/–—]+)\\s*\\)?\\s*of\\s*(\\d{4})\", re.IGNORECASE),\n",
    "    re.compile(r\"No\\.?\\s*[:\\-]?\\s*([A-Za-z0-9\\s\\-/–—]+)\", re.IGNORECASE),\n",
    "]\n",
    "TOKEN_RE = re.compile(r\"\\S+\\s*\")\n",
    "\n",
    "NUMBER_WORDS = {\n",
    "    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\",\n",
    "    \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\",\n",
    "    \"twelve\": \"12\", \"thirteen\": \"13\", \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\",\n",
    "    \"seventeen\": \"17\", \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\",\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\",\n",
    "    \"sixth\": \"6\", \"seventh\": \"7\", \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\"\n",
    "}\n",
    "ROMAN_MAP = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n",
    "DOC_KEYWORDS = [\n",
    "    \"decree-law\", \"decree\", \"decision\", \"resolution\", \"tender\", \"notice\", \"law\",\n",
    "    \"memorandum\", \"circular\", \"announcement\", \"order\", \"statement\", \"invitation\",\n",
    "    \"award\", \"regulation\", \"contract\", \"agreement\", \"ministerial decision\",\n",
    "    \"council\", \"committee\", \"correction\", \"report\"\n",
    "]\n",
    "\n",
    "\n",
    "def _parse_int(value):\n",
    "    try:\n",
    "        text = str(value).strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return int(text)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _roman_to_int(token: str) -> int | None:\n",
    "    total = 0\n",
    "    prev = 0\n",
    "    for ch in token.upper():\n",
    "        if ch not in ROMAN_MAP:\n",
    "            return None\n",
    "        value = ROMAN_MAP[ch]\n",
    "        if value > prev:\n",
    "            total += value - 2 * prev\n",
    "        else:\n",
    "            total += value\n",
    "        prev = value\n",
    "    return total\n",
    "\n",
    "\n",
    "def parse_publication_header(text: str, filename: str) -> dict:\n",
    "    slug = Path(filename).stem\n",
    "    metadata = {}\n",
    "    match = METADATA_RE.search(text)\n",
    "    if match:\n",
    "        try:\n",
    "            metadata = json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            metadata = {}\n",
    "\n",
    "    header_slice = text[:3000]\n",
    "    header_plain = re.sub(r\"<[^>]+>\", \" \", header_slice)\n",
    "\n",
    "    publication_date = metadata.get(\"document_date\") or metadata.get(\"publication_date\")\n",
    "    if not publication_date:\n",
    "        date_match = DATE_FROM_FILENAME_RE.search(slug)\n",
    "        if date_match:\n",
    "            publication_date = \"-\".join(date_match.groups())\n",
    "\n",
    "    issue_number = _parse_int(metadata.get(\"issue_number\"))\n",
    "    if issue_number is None:\n",
    "        issue_match = ISSUE_RE.search(header_plain)\n",
    "        if issue_match:\n",
    "            issue_number = _parse_int(issue_match.group(1))\n",
    "\n",
    "    volume_number = _parse_int(metadata.get(\"volume_number\"))\n",
    "    if volume_number is None and \"Seventy-First\" in header_plain:\n",
    "        volume_number = 71\n",
    "\n",
    "    title = metadata.get(\"document_title\")\n",
    "\n",
    "    supplement_index = 0\n",
    "    sup_match = SUPPLEMENT_NUMBER_RE.search(title or \"\") or SUPPLEMENT_NUMBER_RE.search(header_plain)\n",
    "    if sup_match:\n",
    "        candidate = _parse_int(sup_match.group(1)) or 0\n",
    "        if issue_number and candidate == issue_number:\n",
    "            supplement_index = 1\n",
    "        elif candidate > 50:\n",
    "            supplement_index = 1\n",
    "        else:\n",
    "            supplement_index = candidate\n",
    "    elif re.search(r\"Supplement\", title or \"\", re.IGNORECASE) or re.search(r\"Supplement\", header_plain, re.IGNORECASE):\n",
    "        supplement_index = 1\n",
    "\n",
    "    publication_date = publication_date or \"Unknown\"\n",
    "    issue_number = issue_number or 0\n",
    "    volume_number = volume_number or 0\n",
    "\n",
    "    publication_key = f\"{volume_number}-{issue_number}-{supplement_index}\"\n",
    "\n",
    "    return {\n",
    "        \"slug\": slug,\n",
    "        \"publication_date\": publication_date,\n",
    "        \"volume_number\": volume_number,\n",
    "        \"issue_number\": issue_number,\n",
    "        \"supplement_index\": supplement_index,\n",
    "        \"is_supplement\": supplement_index > 0,\n",
    "        \"title\": title,\n",
    "        \"publication_key\": publication_key,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "\n",
    "\n",
    "def _normalize_doc_number(base: str, year: str | None) -> str:\n",
    "    for char in \"\\u2013\\u2014\\u2012\\u2212\":\n",
    "        base = base.replace(char, \"-\")\n",
    "    base = re.sub(r\"\\s+\", \"\", base.strip())\n",
    "    if year and year not in base:\n",
    "        base = f\"{base}/{year}\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def _extract_doc_number(text: str) -> str | None:\n",
    "    for pattern in DOC_NUMBER_PATTERNS:\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            base = match.group(1)\n",
    "            year = match.group(2) if len(match.groups()) >= 2 else None\n",
    "            return _normalize_doc_number(base, year)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _determine_doc_type(header: str) -> str:\n",
    "    lower = header.lower()\n",
    "    mapping = [\n",
    "        (r\"ministerial decision\", \"Decision\"),\n",
    "        (r\"decree-law\", \"Decree-Law\"),\n",
    "        (r\"emiri decree\", \"Decree\"),\n",
    "        (r\"amiri decree\", \"Decree\"),\n",
    "        (r\"decree\", \"Decree\"),\n",
    "        (r\"resolution\", \"Resolution\"),\n",
    "        (r\"decision\", \"Decision\"),\n",
    "        (r\"memorandum\", \"Memorandum\"),\n",
    "        (r\"circular\", \"Circular\"),\n",
    "        (r\"tender\", \"Tender\"),\n",
    "        (r\"notice\", \"Notice\"),\n",
    "        (r\"invitation\", \"Invitation\"),\n",
    "        (r\"announcement\", \"Announcement\"),\n",
    "        (r\"law\", \"Law\"),\n",
    "        (r\"order\", \"Order\"),\n",
    "        (r\"statement\", \"Statement\"),\n",
    "        (r\"regulation\", \"Regulation\"),\n",
    "        (r\"contract\", \"Contract\"),\n",
    "        (r\"agreement\", \"Agreement\"),\n",
    "        (r\"award\", \"Award\"),\n",
    "        (r\"correction\", \"Correction\"),\n",
    "        (r\"report\", \"Report\")\n",
    "    ]\n",
    "    for pattern, doc_type in mapping:\n",
    "        if re.search(pattern, lower):\n",
    "            return doc_type\n",
    "    return \"Document\"\n",
    "\n",
    "\n",
    "def _detect_title(lines: list[str]) -> str | None:\n",
    "    for line in lines:\n",
    "        stripped = line.strip().lstrip(\"#\").strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        lowered = stripped.lower()\n",
    "        if lowered.startswith(\"no.\"):\n",
    "            continue\n",
    "        if lowered.startswith(\"article\") or lowered.startswith(\"مادة\"):\n",
    "            continue\n",
    "        return stripped\n",
    "    return None\n",
    "\n",
    "\n",
    "def _detect_doc_start(line: str) -> bool:\n",
    "    stripped = line.lstrip()\n",
    "    hashes = len(stripped) - len(stripped.lstrip(\"#\"))\n",
    "    if hashes != 2:\n",
    "        return False\n",
    "    header = stripped.lstrip(\"#\").strip().lower()\n",
    "    return any(keyword in header for keyword in DOC_KEYWORDS)\n",
    "\n",
    "\n",
    "def iter_documents(text: str):\n",
    "    cleaned = BLOCK_RE.sub(\"\", text)\n",
    "    lines = cleaned.splitlines()\n",
    "    current_section = None\n",
    "    pending_pages: list[int] = []\n",
    "    current_doc = None\n",
    "\n",
    "    def flush(doc_state):\n",
    "        if not doc_state:\n",
    "            return None\n",
    "        doc_lines = doc_state[\"lines\"]\n",
    "        body = \"\\n\".join(doc_lines).strip()\n",
    "        if not body:\n",
    "            return None\n",
    "        header = doc_state[\"header\"]\n",
    "        doc_type = _determine_doc_type(header)\n",
    "        doc_number = _extract_doc_number(\"\\n\".join(doc_lines[:5]))\n",
    "        title = _detect_title(doc_lines[1:6])\n",
    "        page_numbers = sorted(set(doc_state.get(\"page_numbers\") or []))\n",
    "        return {\n",
    "            \"doc_type\": doc_type,\n",
    "            \"doc_number\": doc_number,\n",
    "            \"issuer\": None,\n",
    "            \"title\": title,\n",
    "            \"section_heading\": doc_state.get(\"section_heading\"),\n",
    "            \"page_start\": page_numbers[0] if page_numbers else None,\n",
    "            \"page_end\": page_numbers[-1] if page_numbers else None,\n",
    "            \"text\": body,\n",
    "        }\n",
    "\n",
    "    for raw_line in lines:\n",
    "        line = raw_line.rstrip()\n",
    "        if not line:\n",
    "            if current_doc:\n",
    "                current_doc[\"lines\"].append(\"\")\n",
    "            continue\n",
    "        page_start = PAGE_START_RE.search(line)\n",
    "        page_end = PAGE_END_RE.search(line)\n",
    "        if page_start:\n",
    "            pending_pages.append(int(page_start.group(1)))\n",
    "            continue\n",
    "        if page_end:\n",
    "            pending_pages.append(int(page_end.group(1)))\n",
    "            continue\n",
    "        stripped = line.lstrip()\n",
    "        hashes = len(stripped) - len(stripped.lstrip(\"#\"))\n",
    "        if hashes == 1:\n",
    "            current_section = stripped.lstrip(\"#\").strip()\n",
    "            continue\n",
    "        if _detect_doc_start(line):\n",
    "            doc_payload = flush(current_doc)\n",
    "            if doc_payload:\n",
    "                yield doc_payload\n",
    "            header = line.lstrip(\"#\").strip()\n",
    "            current_doc = {\n",
    "                \"header\": header,\n",
    "                \"lines\": [header],\n",
    "                \"section_heading\": current_section,\n",
    "                \"page_numbers\": list(pending_pages) if pending_pages else [],\n",
    "            }\n",
    "            pending_pages.clear()\n",
    "            continue\n",
    "        if current_doc is None:\n",
    "            continue\n",
    "        if pending_pages:\n",
    "            current_doc[\"page_numbers\"].extend(pending_pages)\n",
    "            pending_pages.clear()\n",
    "        current_doc[\"lines\"].append(line)\n",
    "\n",
    "    final_doc = flush(current_doc)\n",
    "    if final_doc:\n",
    "        yield final_doc\n",
    "\n",
    "\n",
    "def _normalize_article_number(token: str | None) -> str | None:\n",
    "    if not token:\n",
    "        return None\n",
    "    cleaned = token.strip(\"()[]{}:.- \").lower()\n",
    "    if not cleaned:\n",
    "        return None\n",
    "    if cleaned in NUMBER_WORDS:\n",
    "        return NUMBER_WORDS[cleaned]\n",
    "    roman = _roman_to_int(cleaned)\n",
    "    if roman is not None:\n",
    "        return str(roman)\n",
    "    digits = re.findall(r\"\\d+\", cleaned)\n",
    "    if digits:\n",
    "        return digits[0]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def _is_article_heading(line: str) -> tuple[bool, str | None]:\n",
    "    stripped = line.strip().lstrip(\"*-#\").strip()\n",
    "    lower = stripped.lower()\n",
    "    if not (lower.startswith(\"article\") or lower.startswith(\"مادة\")):\n",
    "        return False, None\n",
    "    parts = stripped.split(None, 1)\n",
    "    if len(parts) < 2:\n",
    "        return False, None\n",
    "    rest = parts[1].strip()\n",
    "    number_match = re.match(r\"^\\(?([A-Za-z0-9IVXLCDM\\-\\u0660-\\u0669]+)\\)?\", rest, re.IGNORECASE)\n",
    "    if not number_match:\n",
    "        return False, None\n",
    "    number_token = number_match.group(1)\n",
    "    after_number = rest[number_match.end():].strip()\n",
    "    if after_number and not after_number.startswith((\":\", \"-\", \"–\", \"—\")):\n",
    "        return False, None\n",
    "    return True, _normalize_article_number(number_token)\n",
    "\n",
    "\n",
    "def _token_chunks(text: str, max_tokens: int = 900, overlap: int = 90) -> list[str]:\n",
    "    tokens = TOKEN_RE.findall(text)\n",
    "    if not tokens:\n",
    "        return []\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [\"\".join(tokens).strip()]\n",
    "    chunks: list[str] = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(len(tokens), start + max_tokens)\n",
    "        chunk = \"\".join(tokens[start:end]).strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def article_aware_chunks(publication: dict, document: dict):\n",
    "    lines = document[\"text\"].splitlines()\n",
    "    sections: list[tuple[str | None, str]] = []\n",
    "    current_lines: list[str] = []\n",
    "    current_article: str | None = None\n",
    "    for line in lines:\n",
    "        is_article, article_number = _is_article_heading(line)\n",
    "        if is_article:\n",
    "            if current_lines:\n",
    "                section_text = \"\\n\".join(current_lines).strip()\n",
    "                if section_text:\n",
    "                    sections.append((current_article, section_text))\n",
    "            current_lines = [line]\n",
    "            current_article = article_number\n",
    "        else:\n",
    "            current_lines.append(line)\n",
    "    if current_lines:\n",
    "        section_text = \"\\n\".join(current_lines).strip()\n",
    "        if section_text:\n",
    "            sections.append((current_article, section_text))\n",
    "    if not sections:\n",
    "        sections = [(None, document[\"text\"].strip())]\n",
    "\n",
    "    header_prefix = f\"[PUB={publication['publication_key']}] [ISSUE={publication['issue_number']}] [SUPP={publication['supplement_index']}] [DATE={publication['publication_date']}]\"\n",
    "    for article_number, section_text in sections:\n",
    "        if not section_text:\n",
    "            continue\n",
    "        bodies = _token_chunks(section_text)\n",
    "        if not bodies:\n",
    "            bodies = [section_text]\n",
    "        for body in bodies:\n",
    "            body = body.strip()\n",
    "            if not body:\n",
    "                continue\n",
    "            doc_number = document.get(\"doc_number\") or \"\"\n",
    "            section_heading = document.get(\"section_heading\") or \"\"\n",
    "            article_token = article_number or \"\"\n",
    "            header_line = f\"[DOC={document['doc_type']} {doc_number}] [SECTION={section_heading}] [ARTICLE={article_token}]\"\n",
    "            yield {\n",
    "                \"chunk_id\": str(uuid.uuid4()),\n",
    "                \"publication_key\": publication[\"publication_key\"],\n",
    "                \"document_key\": document[\"document_key\"],\n",
    "                \"article_number\": article_number,\n",
    "                \"text\": f\"{header_prefix}\\n{header_line}\\n{body}\",\n",
    "            }\n",
    "\n",
    "\n",
    "def ensure_constraints(graph_client):\n",
    "    queries = [\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT publication_key_unique IF NOT EXISTS\n",
    "        FOR (p:Publication) REQUIRE p.publication_key IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT document_key_unique IF NOT EXISTS\n",
    "        FOR (d:Document) REQUIRE d.document_key IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS\n",
    "        FOR (c:Chunk) REQUIRE c.chunk_id IS UNIQUE\n",
    "        \"\"\",\n",
    "    ]\n",
    "    for query in queries:\n",
    "        graph_client.query(query)\n",
    "\n",
    "ensure_constraints(graph)\n",
    "\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "    MERGE (g:Gazette {name: $name})\n",
    "    ON CREATE SET g.name = $name\n",
    "    \"\"\",\n",
    "    {\"name\": GAZETTE_NAME},\n",
    ")\n",
    "\n",
    "\n",
    "def _sanitize_doc_type(doc_type: str) -> str:\n",
    "    safe = re.sub(r\"\\s+\", \"-\", doc_type.strip())\n",
    "    return safe or \"Document\"\n",
    "\n",
    "md_files = sorted(SOURCE_DIR.rglob(\"*.md\"))\n",
    "if not md_files:\n",
    "    tqdm.write(\"No markdown files found for ingestion.\", file=sys.stderr)\n",
    "\n",
    "prepared_publications = []\n",
    "total_chunks = 0\n",
    "for path in md_files:\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    publication = parse_publication_header(text, str(path))\n",
    "    documents = []\n",
    "    doc_seq = 0\n",
    "    for doc in iter_documents(text):\n",
    "        doc_seq += 1\n",
    "        doc_type = doc[\"doc_type\"] or \"Document\"\n",
    "        key_doc_type = _sanitize_doc_type(doc_type)\n",
    "        doc_number = doc.get(\"doc_number\")\n",
    "        key_suffix = doc_number or f\"{doc_seq:03d}\"\n",
    "        document_key = f\"{publication['publication_key']}:{key_doc_type}:{key_suffix}\"\n",
    "        doc[\"document_key\"] = document_key\n",
    "        doc[\"doc_type\"] = doc_type\n",
    "        doc[\"doc_sequence\"] = doc_seq\n",
    "        documents.append(doc)\n",
    "    for doc in documents:\n",
    "        chunks = list(article_aware_chunks(publication, doc))\n",
    "        doc[\"chunks\"] = chunks\n",
    "        total_chunks += len(chunks)\n",
    "    prepared_publications.append({\n",
    "        \"path\": path,\n",
    "        \"publication\": publication,\n",
    "        \"documents\": documents,\n",
    "    })\n",
    "    tqdm.write(\n",
    "        f\"Prepared {path.name}: {len(documents)} documents, {sum(len(d['chunks']) for d in documents)} chunks\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n",
    "try:\n",
    "    term_width = shutil.get_terminal_size().columns\n",
    "except Exception:\n",
    "    term_width = 100\n",
    "\n",
    "\n",
    "def _ingest_publication_entry(entry, bar):\n",
    "    publication = entry[\"publication\"]\n",
    "    publication_props = {\n",
    "        \"slug\": publication[\"slug\"],\n",
    "        \"title\": publication.get(\"title\"),\n",
    "        \"publication_date\": publication[\"publication_date\"],\n",
    "        \"volume_number\": publication[\"volume_number\"],\n",
    "        \"issue_number\": publication[\"issue_number\"],\n",
    "        \"supplement_index\": publication[\"supplement_index\"],\n",
    "        \"is_supplement\": publication[\"is_supplement\"],\n",
    "        \"source_path\": str(entry[\"path\"]),\n",
    "    }\n",
    "\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE (g:Gazette {name: $gazette_name})\n",
    "        WITH g\n",
    "        MERGE (p:Publication {publication_key: $publication_key})\n",
    "        ON CREATE SET p += $props\n",
    "        ON MATCH SET p += $props\n",
    "        MERGE (g)-[:HAS_PUBLICATION]->(p)\n",
    "        \"\"\",\n",
    "        {\n",
    "            \"gazette_name\": GAZETTE_NAME,\n",
    "            \"publication_key\": publication[\"publication_key\"],\n",
    "            \"props\": publication_props,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def _ingest_one_doc(doc):\n",
    "        doc_props = {\n",
    "            \"doc_type\": doc[\"doc_type\"],\n",
    "            \"doc_number\": doc.get(\"doc_number\"),\n",
    "            \"issuer\": doc.get(\"issuer\"),\n",
    "            \"title\": doc.get(\"title\"),\n",
    "            \"section_heading\": doc.get(\"section_heading\"),\n",
    "            \"page_start\": doc.get(\"page_start\"),\n",
    "            \"page_end\": doc.get(\"page_end\"),\n",
    "            \"doc_sequence\": doc.get(\"doc_sequence\"),\n",
    "            \"publication_key\": publication[\"publication_key\"],\n",
    "        }\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            MATCH (p:Publication {publication_key: $publication_key})\n",
    "            MERGE (d:Document {document_key: $document_key})\n",
    "            ON CREATE SET d += $props\n",
    "            ON MATCH SET d += $props\n",
    "            MERGE (p)-[:CONTAINS]->(d)\n",
    "            \"\"\",\n",
    "            {\n",
    "                \"publication_key\": publication[\"publication_key\"],\n",
    "                \"document_key\": doc[\"document_key\"],\n",
    "                \"props\": doc_props,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        chunks = doc.get(\"chunks\") or []\n",
    "        if chunks:\n",
    "            graph.query(\n",
    "                \"\"\"\n",
    "                UNWIND $chunks AS chunk\n",
    "                MERGE (c:Chunk {chunk_id: chunk.chunk_id})\n",
    "                ON CREATE SET c += chunk\n",
    "                ON MATCH SET c += chunk\n",
    "                WITH c, chunk\n",
    "                MATCH (d:Document {document_key: chunk.document_key})\n",
    "                MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "                \"\"\",\n",
    "                {\"chunks\": chunks},\n",
    "            )\n",
    "        return len(chunks)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_ingest_one_doc, d) for d in entry[\"documents\"]]\n",
    "        for fut in futures:\n",
    "            try:\n",
    "                count = fut.result()\n",
    "                if count:\n",
    "                    bar.update(count)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Doc ingest error: {e}\", file=sys.stderr)\n",
    "\n",
    "start = time.perf_counter()\n",
    "with tqdm(\n",
    "    total=max(total_chunks, 1),\n",
    "    desc=f\"Ingesting chunks (workers={max_workers})\",\n",
    "    ncols=term_width,\n",
    "    bar_format=bar_format,\n",
    "    file=sys.stderr,\n",
    "    disable=(total_chunks == 0),\n",
    ") as bar:\n",
    "    for entry in prepared_publications:\n",
    "        _ingest_publication_entry(entry, bar)\n",
    "\n",
    "dt = time.perf_counter() - start\n",
    "if total_chunks == 0:\n",
    "    tqdm.write(\"No chunks were generated for ingestion.\", file=sys.stderr)\n",
    "else:\n",
    "    rate = total_chunks / max(dt, 1e-6)\n",
    "    tqdm.write(f\"Ingestion complete. Ingested {total_chunks} chunks in {dt:.1f}s ({rate:,.0f} chunks/s).\", file=sys.stderr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Core Counts ---\n",
      "Gazettes: 1\n",
      "Publications: 10 (expected 10)\n",
      "Documents: 567\n",
      "Chunks: 2773\n",
      "\n",
      "--- Supplement Spot Checks ---\n",
      "Issue 1736 -> 71-1736-0 (supplement=False, index=0)\n",
      "Issue 1736 -> 71-1736-2 (supplement=True, index=2)\n",
      "Issue 1738 -> 71-1738-0 (supplement=False, index=0)\n",
      "Issue 1738 -> 71-1738-1 (supplement=True, index=1)\n",
      "\n",
      "--- Document Key Samples ---\n",
      "71-1733-0 :: 71-1733-0:Announcement:uncement\n",
      "71-1733-0 :: 71-1733-0:Announcement:uncementArequesthasbeensubmittedtotheDepartmentofPartnerships\n",
      "71-1733-0 :: 71-1733-0:Announcement:uncementMessrs\n",
      "\n",
      "--- Chunk Header Preview ---\n",
      "Chunk 1: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] | [DOC=Decree 71/2025] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] [ARTICLE=1]\n",
      "Chunk 2: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] | [DOC=Decree 71/2025] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] [ARTICLE=1]\n",
      "Chunk 3: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] | [DOC=Decree 71/2025] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] [ARTICLE=]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Summary & validation checks\n",
    "# -------------------------------------------------------------\n",
    "from textwrap import dedent\n",
    "\n",
    "def run_query(query, params=None):\n",
    "    return graph.query(query, params or {})\n",
    "\n",
    "expected_publications = len(md_files) if 'md_files' in globals() else None\n",
    "\n",
    "print('\\n--- Core Counts ---')\n",
    "gazette_count = run_query('MATCH (g:Gazette) RETURN count(g) AS c')[0]['c']\n",
    "publication_count = run_query('MATCH (p:Publication) RETURN count(p) AS c')[0]['c']\n",
    "document_count = run_query('MATCH (d:Document) RETURN count(d) AS c')[0]['c']\n",
    "chunk_count = run_query('MATCH (c:Chunk) RETURN count(c) AS c')[0]['c']\n",
    "print(f'Gazettes: {gazette_count}')\n",
    "if expected_publications is not None:\n",
    "    print(f'Publications: {publication_count} (expected {expected_publications})')\n",
    "else:\n",
    "    print(f'Publications: {publication_count}')\n",
    "print(f'Documents: {document_count}')\n",
    "print(f'Chunks: {chunk_count}')\n",
    "\n",
    "print('\\n--- Supplement Spot Checks ---')\n",
    "for issue in (1736, 1738):\n",
    "    rows = run_query(\n",
    "        'MATCH (p:Publication {issue_number: $issue}) RETURN p.publication_key AS key, p.is_supplement AS is_supplement, p.supplement_index AS supplement_index ORDER BY p.supplement_index',\n",
    "        {'issue': issue},\n",
    "    )\n",
    "    if not rows:\n",
    "        print(f'Issue {issue}: no rows found')\n",
    "        continue\n",
    "    for row in rows:\n",
    "        print(f\"Issue {issue} -> {row['key']} (supplement={row['is_supplement']}, index={row['supplement_index']})\")\n",
    "\n",
    "print('\\n--- Document Key Samples ---')\n",
    "for row in run_query(\n",
    "    'MATCH (p:Publication)-[:CONTAINS]->(d:Document) RETURN p.publication_key AS pk, d.document_key AS dk ORDER BY d.document_key LIMIT 3'\n",
    "):\n",
    "    print(f\"{row['pk']} :: {row['dk']}\")\n",
    "\n",
    "print('\\n--- Chunk Header Preview ---')\n",
    "chunk_rows = run_query('MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk) RETURN c.text AS text LIMIT 3')\n",
    "if not chunk_rows:\n",
    "    print('No chunks found.')\n",
    "else:\n",
    "    for idx, row in enumerate(chunk_rows, start=1):\n",
    "        header_lines = row['text'].splitlines()[:2]\n",
    "        preview = ' | '.join(header_lines)\n",
    "        print(f'Chunk {idx}: {preview[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplement links: 2, latest publications: 8, tagged chunks: 1781\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Supplements + latest flags + authority weights\n",
    "# -------------------------------------------------------------\n",
    "from textwrap import dedent\n",
    "\n",
    "def run(q, params=None):\n",
    "    return graph.query(q, params or {})\n",
    "\n",
    "# Link supplements to their base (same volume + issue; supplement_index>0)\n",
    "run(dedent('''\n",
    "MATCH (base:Publication {supplement_index: 0})\n",
    "MATCH (sup:Publication)\n",
    "WHERE sup.volume_number = base.volume_number\n",
    "  AND sup.issue_number = base.issue_number\n",
    "  AND sup.supplement_index > 0\n",
    "MERGE (sup)-[:SUPPLEMENT_OF]->(base)\n",
    "'''))\n",
    "\n",
    "# Mark latest per (volume, issue) and set a grouping key\n",
    "run(dedent('''\n",
    "MATCH (p:Publication)\n",
    "WITH p.volume_number AS v, p.issue_number AS i, max(p.supplement_index) AS maxSupp\n",
    "MATCH (q:Publication {volume_number: v, issue_number: i})\n",
    "SET q.is_latest_for_issue = (q.supplement_index = maxSupp),\n",
    "    q.issue_group_key = toString(v) + '-' + toString(i)\n",
    "'''))\n",
    "\n",
    "# Tag chunks from the latest publication for each issue\n",
    "run(dedent('''\n",
    "MATCH (p:Publication {is_latest_for_issue: true})-[:CONTAINS]->(:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "SET c.latest_for_issue = true\n",
    "'''))\n",
    "\n",
    "# Mirror supplement flag onto chunks\n",
    "run(dedent('''\n",
    "MATCH (p:Publication)-[:CONTAINS]->(:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "SET c.is_supplement = p.is_supplement\n",
    "'''))\n",
    "\n",
    "# Set authority weights by doc_type (simple heuristic)\n",
    "run(dedent('''\n",
    "MATCH (d:Document)\n",
    "SET d.authority_weight =\n",
    "  CASE d.doc_type\n",
    "    WHEN 'Decree-Law' THEN 1.30\n",
    "    WHEN 'Decree' THEN 1.20\n",
    "    WHEN 'Law' THEN 1.20\n",
    "    WHEN 'Decision' THEN 1.10\n",
    "    WHEN 'Resolution' THEN 1.05\n",
    "    WHEN 'Memorandum' THEN 1.00\n",
    "    WHEN 'Notice' THEN 0.95\n",
    "    WHEN 'Tender' THEN 0.90\n",
    "    ELSE 1.00\n",
    "  END\n",
    "'''))\n",
    "\n",
    "# Quick summary\n",
    "sup_links = run('MATCH (:Publication)-[:SUPPLEMENT_OF]->(:Publication) RETURN count(*) AS n')[0]['n']\n",
    "latest_pubs = run('MATCH (p:Publication {is_latest_for_issue:true}) RETURN count(p) AS n')[0]['n']\n",
    "latest_chunks = run('MATCH (c:Chunk {latest_for_issue:true}) RETURN count(c) AS n')[0]['n']\n",
    "print(f\"Supplement links: {sup_links}, latest publications: {latest_pubs}, tagged chunks: {latest_chunks}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First ingest all data and then create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Anchor extraction + indexes (minimal, persistent) ---\n",
    "import re\n",
    "\n",
    "_ARABIC_DIGITS = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "\n",
    "def _norm_digits(s: str) -> str:\n",
    "    return (s or \"\").translate(_ARABIC_DIGITS)\n",
    "\n",
    "_RX_ANCHORS = [\n",
    "    re.compile(r\"\\bRFP[\\s/-]?\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b\\d{6,7}[\\s/-]?RFP\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bA/M/\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b5D[A-Z0-9]+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b\\d{4}/\\d{5}\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bMEETING (?:MINUTES )?NO\\.?\\s*\\d{4}/\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b(?:DECREE-?LAW|LAW)\\s*(?:NO\\.?\\s*)?\\d+/?\\d{4}\\b\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extract_anchors_from_text(text: str) -> list[str]:\n",
    "    t = _norm_digits(text or \"\")\n",
    "    out: list[str] = []\n",
    "    for rx in _RX_ANCHORS:\n",
    "        out.extend(rx.findall(t))\n",
    "    seen = set(); uniq: list[str] = []\n",
    "    for a in out:\n",
    "        k = a.upper()\n",
    "        if k not in seen:\n",
    "            seen.add(k); uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "# Compute anchors per Document from its chunks and persist\n",
    "rows = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "    RETURN d.document_key AS dk, collect(c.text) AS texts\n",
    "    \"\"\"\n",
    ")\n",
    "for row in rows:\n",
    "    anchors = set()\n",
    "    for t in row[\"texts\"]:\n",
    "        anchors.update(extract_anchors_from_text(t))\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document {document_key: $dk})\n",
    "        SET d.anchors = $anchors\n",
    "        \"\"\",\n",
    "        {\"dk\": row[\"dk\"], \"anchors\": sorted(list(anchors))},\n",
    "    )\n",
    "\n",
    "# Helpful fulltext/property indexes (idempotent)\n",
    "run = (lambda q, p=None: graph.query(q, p or {}))\n",
    "run(\"CREATE FULLTEXT INDEX chunk_text_idx IF NOT EXISTS FOR (c:Chunk) ON EACH [c.text]\")\n",
    "run(\"CREATE FULLTEXT INDEX doc_text_idx   IF NOT EXISTS FOR (d:Document) ON EACH [d.title, d.section_heading, d.doc_number, d.document_key]\")\n",
    "run(\"CREATE FULLTEXT INDEX doc_anchors_idx IF NOT EXISTS FOR (d:Document) ON EACH [d.anchors]\")\n",
    "run(\"CREATE INDEX doc_pub_key_idx IF NOT EXISTS FOR (d:Document) ON (d.publication_key)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding update...\n",
      "Found 2773 nodes without embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding nodes: 100%|██████████████████████████████████████████| 2773/2773 [00:29<00:00, 93.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding update.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Vector Index\n",
    "create_vector_index(graph=graph, index_name='Chunk')\n",
    "\n",
    "# Embed Chunks\n",
    "embed_text(graph=graph, OPENAI_API_KEY=openAI_api, OPENAI_ENDPOINT=openAI_endpoint, node_name='Chunk', model_name=openAI_model, max_workers=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag-developer-challenge3-4suqu0LP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
