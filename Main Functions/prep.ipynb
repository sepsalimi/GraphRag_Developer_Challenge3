{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sepeh/.cache/pypoetry/virtualenvs/graphrag-developer-challenge3-4suqu0LP-py3.12/bin/python\n",
      "Kernel working. Proceed\n"
     ]
    }
   ],
   "source": [
    "# Confirm Jupyter and kernel works\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(\"Kernel working. Proceed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3/Main Functions\n",
      "Parent directory: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3\n",
      "Parent dir exists: True\n",
      "KnowledgeGraph exists: True\n",
      "Successfully connected to Neo4j!\n"
     ]
    }
   ],
   "source": [
    "# Connect to Neo4j\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add parent directory to Python path so we can import KnowledgeGraph\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Parent directory: {parent_dir}\")\n",
    "print(f\"Parent dir exists: {os.path.exists(parent_dir)}\")\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "print(f\"KnowledgeGraph exists: {os.path.exists(os.path.join(parent_dir, 'KnowledgeGraph'))}\")\n",
    "\n",
    "from KnowledgeGraph.knowledgegraph import ingest_Chunks, create_nodes, create_relationship, create_vector_index, embed_text\n",
    "from KnowledgeGraph.chunking_strategy import ChunkingConfig, chunk_document_text\n",
    "from KnowledgeGraph.config import load_neo4j_graph\n",
    "\n",
    "import json\n",
    "\n",
    "# Error handling for Neo4j connection\n",
    "try:\n",
    "    graph, openAI_api, openAI_endpoint, openAI_model = load_neo4j_graph()\n",
    "    print(\"Successfully connected to Neo4j!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set up Neo4j (local or AuraDB) and update .env file\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting old Chunk, Document, Publication, Gazette, Issue, Issues, Page, Section nodes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old nodes deleted successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Cleanup (optional reset between runs)\n",
    "# -------------------------------------------------------------\n",
    "labels_to_wipe = ['Chunk', 'Document', 'Publication', 'Gazette', 'Issue', 'Issues', 'Page', 'Section']\n",
    "print(f\"Deleting old {', '.join(labels_to_wipe)} nodes...\")\n",
    "try:\n",
    "    graph.query(\"\"\"\n",
    "    MATCH (n)\n",
    "    WHERE n:Chunk OR n:Document OR n:Publication OR n:Gazette OR n:Issue OR n:Issues OR n:Page OR n:Section\n",
    "    DETACH DELETE n\n",
    "    \"\"\")\n",
    "    print(\"Old nodes deleted successfully.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Cleanup skipped: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <r> Clear Embeddings </r> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped index `Chunk` (if it existed).\n",
      "Cleared old embeddings from Chunk nodes.\n",
      "Recreated vector index `Chunk` with dims from OPENAI_EMBED_DIM.\n",
      "Index info: {'name': 'Chunk', 'options': {'indexProvider': 'vector-2.0', 'indexConfig': {'vector.hnsw.m': 16, 'vector.hnsw.ef_construction': 100, 'vector.dimensions': 3072, 'vector.similarity_function': 'COSINE', 'vector.quantization.enabled': True}}}\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Recreate vector index at dims from .env and clear old vectors\n",
    "# -------------------------------------------------------------\n",
    "from textwrap import dedent\n",
    "\n",
    "def run(q, params=None):\n",
    "    return graph.query(q, params or {})\n",
    "\n",
    "# 1) Drop old index (idempotent)\n",
    "try:\n",
    "    run(\"DROP INDEX `Chunk` IF EXISTS\")\n",
    "    print(\"Dropped index `Chunk` (if it existed).\")\n",
    "except Exception as e:\n",
    "    print(f\"Drop index skipped: {e}\")\n",
    "\n",
    "# 2) Clear old vectors (they may be wrong dims)\n",
    "run(\"MATCH (c:Chunk) REMOVE c.textEmbeddingOpenAI\")\n",
    "print(\"Cleared old embeddings from Chunk nodes.\")\n",
    "\n",
    "# 3) Recreate index using dims from .env (OPENAI_EMBED_DIM)\n",
    "from KnowledgeGraph.knowledgegraph import create_vector_index\n",
    "create_vector_index(graph=graph, index_name='Chunk')  # reads OPENAI_EMBED_DIM\n",
    "print(\"Recreated vector index `Chunk` with dims from OPENAI_EMBED_DIM.\")\n",
    "\n",
    "# 4) Show index options to confirm dims\n",
    "rows = run(dedent(\"\"\"\n",
    "SHOW INDEXES\n",
    "YIELD name, type, entityType, labelsOrTypes, properties, options\n",
    "WHERE name = 'Chunk'\n",
    "RETURN name, options\n",
    "\"\"\"))\n",
    "print(\"Index info:\", rows[0] if rows else \"Index not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop through each file and ingest main nodes and chunk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Project root: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3/Main Functions\n",
      "Source folder: /mnt/c/Users/sepeh/OneDrive/Documents/Git/GraphRag_Developer_Challenge3/KnowledgeGraph/source_data\n",
      "Chunk preview: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] [SRC=2025-04-27_en.md] [DOC=Decree 71/2025] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] | Decree No. 71 of 2025\n",
      "Prepared 2025-04-27_en.md: 115 documents, 1136 chunks\n",
      "Chunk preview: [PUB=71-1736-2] [ISSUE=1736] [SUPP=2] [DATE=2025-05-01] [SRC=2025-05-01_en.md] [DOC=Decree-Law 63/2025] [SECTION=Council of Ministers] | Decree-Law No. 63 of 2025\n",
      "Prepared 2025-05-01_en.md: 1 documents, 9 chunks\n",
      "Chunk preview: [PUB=71-1737-0] [ISSUE=1737] [SUPP=0] [DATE=2025-05-04] [SRC=2025-05-04_en.md] [DOC=Resolution 75/2025] [SECTION=Capital Markets Authority / Hay'at Aswaq Al-Mal <orig>هيئة أسواق المال</orig>] | Resolution No. (75) of 2025 | Regarding the Delisting of Shares of Jeiad Holding Company <orig>شركة جياد القابضة</orig> | (K.S.C.P.)\n",
      "Prepared 2025-05-04_en.md: 144 documents, 1511 chunks\n",
      "Chunk preview: [PUB=71-1738-0] [ISSUE=1738] [SUPP=0] [DATE=2025-05-11] [SRC=2025-05-11_en.md] [DOC=Decree-Law 65/2025] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] | Decree-Law No. 65 of 2025\n",
      "Prepared 2025-05-11_en.md: 182 documents, 1702 chunks\n",
      "Chunk preview: [PUB=71-1738-1] [ISSUE=1738] [SUPP=1] [DATE=2025-05-15] [SRC=2025-05-15_en.md] [DOC=Decision 9/78-2/2025] [SECTION=Council of Ministers] | Decision of the Supreme Committee for the Verification of Kuwaiti Nationality\n",
      "Prepared 2025-05-15_en.md: 2 documents, 12 chunks\n",
      "Chunk preview: [PUB=71-1739-1] [ISSUE=1739] [SUPP=1] [DATE=2025-05-20] [SRC=2025-05-20_en.md] [DOC=Decision ] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] | Decision of the Supreme Committee for the Verification of Kuwaiti Nationality <orig>اللجنة العليا لتحقيق الجنسية الكويتية</orig>\n",
      "Prepared 2025-05-20_en.md: 1 documents, 6 chunks\n",
      "Chunk preview: [PUB=71-1740-0] [ISSUE=1740] [SUPP=0] [DATE=2025-05-25] [SRC=2025-05-25_en.md] [DOC=Decree ] [SECTION=Contents of the Issue] | Part One (Judgments, Laws, Decrees, and Resolutions) | - Decree-Laws (A1-2) | - Decrees (A2-7)\n",
      "Prepared 2025-05-25_en.md: 113 documents, 999 chunks\n",
      "Chunk preview: [PUB=71-1733-0] [ISSUE=1733] [SUPP=0] [DATE=2025-04-06] [SRC=2025_04_06_en.md] [DOC=Decree-Law 61/2025] [SECTION=Council of Ministers / Majlis Al-Wuzara <orig>مجلس الوزراء</orig>] | Decree-Law No. 61 of 2025\n",
      "Prepared 2025_04_06_en.md: 88 documents, 732 chunks\n",
      "Chunk preview: [PUB=71-1734-0] [ISSUE=1734] [SUPP=0] [DATE=2025-04-13] [SRC=2025_04_13_en.md] [DOC=Decree 63/2025] [SECTION=Council of Ministers <orig>مجلس الوزراء</orig>] | Decree No. 63 of 2025\n",
      "Prepared 2025_04_13_en.md: 135 documents, 1449 chunks\n",
      "Chunk preview: [PUB=71-1735-0] [ISSUE=1735] [SUPP=0] [DATE=2025-04-20] [SRC=2025_04_20_en.md] [DOC=Decree-Law 62/2025] [SECTION=Council of Ministers] | Decree-Law No. 62 of 2025\n",
      "Prepared 2025_04_20_en.md: 126 documents, 932 chunks\n",
      "Ingesting chunks (workers=16): 100%|███████████████████| 8488/8488 [00:18<00:00]\n",
      "Ingestion complete. Ingested 8488 chunks in 18.3s (463 chunks/s).\n"
     ]
    }
   ],
   "source": [
    "import json, re, uuid, shutil, time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path(os.getcwd())\n",
    "SOURCE_DIR = (PROJECT_ROOT.parent / \"KnowledgeGraph\" / \"source_data\").resolve()\n",
    "GAZETTE_NAME = \"Kuwait Today / Al-Kuwait Al-Youm\"\n",
    "CHUNK_CONFIG = ChunkingConfig()\n",
    "max_workers = int(os.getenv(\"MAX_WORKERS\", str(min(16, (os.cpu_count() or 4) * 2))))\n",
    "\n",
    "if not SOURCE_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Expected source markdown directory at {SOURCE_DIR}\")\n",
    "\n",
    "tqdm.write(f\"Project root: {PROJECT_ROOT}\", file=sys.stderr)\n",
    "tqdm.write(f\"Source folder: {SOURCE_DIR}\", file=sys.stderr)\n",
    "\n",
    "METADATA_RE = re.compile(r\"<document_metadata>\\s*(\\{.*?\\})\\s*</document_metadata>\", re.DOTALL)\n",
    "DATE_FROM_FILENAME_RE = re.compile(r\"(\\d{4})[-_](\\d{2})[-_](\\d{2})\")\n",
    "SUPPLEMENT_NUMBER_RE = re.compile(r\"Supplement\\s*(?:No\\.?|Number)?\\s*(\\d+)\", re.IGNORECASE)\n",
    "ISSUE_RE = re.compile(r\"Issue\\s*(?:No\\.?|Number)?\\s*(\\d{3,4})\", re.IGNORECASE)\n",
    "BLOCK_RE = re.compile(r\"<(document_metadata|page_metadata)>.*?</\\1>\", re.DOTALL)\n",
    "PAGE_START_RE = re.compile(r\"<page_start[^>]*>(\\d+)</page_start>\")\n",
    "PAGE_END_RE = re.compile(r\"<page_end[^>]*>(\\d+)</page_end>\")\n",
    "DOC_NUMBER_PATTERNS = [\n",
    "    re.compile(r\"No\\.?\\s*\\(?\\s*([A-Za-z0-9\\s\\-/–—]+)\\s*\\)?\\s*of\\s*(\\d{4})\", re.IGNORECASE),\n",
    "    re.compile(r\"No\\.?\\s*[:\\-]?\\s*([A-Za-z0-9\\s\\-/–—]+)\", re.IGNORECASE),\n",
    "]\n",
    "TOKEN_RE = re.compile(r\"\\S+\\s*\")\n",
    "\n",
    "NUMBER_WORDS = {\n",
    "    \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\", \"five\": \"5\",\n",
    "    \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\", \"ten\": \"10\", \"eleven\": \"11\",\n",
    "    \"twelve\": \"12\", \"thirteen\": \"13\", \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\",\n",
    "    \"seventeen\": \"17\", \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\",\n",
    "    \"first\": \"1\", \"second\": \"2\", \"third\": \"3\", \"fourth\": \"4\", \"fifth\": \"5\",\n",
    "    \"sixth\": \"6\", \"seventh\": \"7\", \"eighth\": \"8\", \"ninth\": \"9\", \"tenth\": \"10\"\n",
    "}\n",
    "ROMAN_MAP = {\"I\": 1, \"V\": 5, \"X\": 10, \"L\": 50, \"C\": 100, \"D\": 500, \"M\": 1000}\n",
    "DOC_KEYWORDS = [\n",
    "    \"decree-law\", \"decree\", \"decision\", \"resolution\", \"tender\", \"notice\", \"law\",\n",
    "    \"memorandum\", \"circular\", \"announcement\", \"order\", \"statement\", \"invitation\",\n",
    "    \"award\", \"regulation\", \"contract\", \"agreement\", \"ministerial decision\",\n",
    "    \"council\", \"committee\", \"correction\", \"report\"\n",
    "]\n",
    "\n",
    "\n",
    "def _parse_int(value):\n",
    "    try:\n",
    "        text = str(value).strip()\n",
    "        if not text:\n",
    "            return None\n",
    "        return int(text)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _roman_to_int(token: str) -> int | None:\n",
    "    total = 0\n",
    "    prev = 0\n",
    "    for ch in token.upper():\n",
    "        if ch not in ROMAN_MAP:\n",
    "            return None\n",
    "        value = ROMAN_MAP[ch]\n",
    "        if value > prev:\n",
    "            total += value - 2 * prev\n",
    "        else:\n",
    "            total += value\n",
    "        prev = value\n",
    "    return total\n",
    "\n",
    "\n",
    "def parse_publication_header(text: str, filename: str) -> dict:\n",
    "    slug = Path(filename).stem\n",
    "    metadata = {}\n",
    "    match = METADATA_RE.search(text)\n",
    "    if match:\n",
    "        try:\n",
    "            metadata = json.loads(match.group(1))\n",
    "        except json.JSONDecodeError:\n",
    "            metadata = {}\n",
    "\n",
    "    header_slice = text[:3000]\n",
    "    header_plain = re.sub(r\"<[^>]+>\", \" \", header_slice)\n",
    "\n",
    "    publication_date = metadata.get(\"document_date\") or metadata.get(\"publication_date\")\n",
    "    if not publication_date:\n",
    "        date_match = DATE_FROM_FILENAME_RE.search(slug)\n",
    "        if date_match:\n",
    "            publication_date = \"-\".join(date_match.groups())\n",
    "\n",
    "    issue_number = _parse_int(metadata.get(\"issue_number\"))\n",
    "    if issue_number is None:\n",
    "        issue_match = ISSUE_RE.search(header_plain)\n",
    "        if issue_match:\n",
    "            issue_number = _parse_int(issue_match.group(1))\n",
    "\n",
    "    volume_number = _parse_int(metadata.get(\"volume_number\"))\n",
    "    if volume_number is None and \"Seventy-First\" in header_plain:\n",
    "        volume_number = 71\n",
    "\n",
    "    title = metadata.get(\"document_title\")\n",
    "\n",
    "    supplement_index = 0\n",
    "    sup_match = SUPPLEMENT_NUMBER_RE.search(title or \"\") or SUPPLEMENT_NUMBER_RE.search(header_plain)\n",
    "    if sup_match:\n",
    "        candidate = _parse_int(sup_match.group(1)) or 0\n",
    "        if issue_number and candidate == issue_number:\n",
    "            supplement_index = 1\n",
    "        elif candidate > 50:\n",
    "            supplement_index = 1\n",
    "        else:\n",
    "            supplement_index = candidate\n",
    "    elif re.search(r\"Supplement\", title or \"\", re.IGNORECASE) or re.search(r\"Supplement\", header_plain, re.IGNORECASE):\n",
    "        supplement_index = 1\n",
    "\n",
    "    publication_date = publication_date or \"Unknown\"\n",
    "    issue_number = issue_number or 0\n",
    "    volume_number = volume_number or 0\n",
    "\n",
    "    publication_key = f\"{volume_number}-{issue_number}-{supplement_index}\"\n",
    "\n",
    "    return {\n",
    "        \"slug\": slug,\n",
    "        \"publication_date\": publication_date,\n",
    "        \"volume_number\": volume_number,\n",
    "        \"issue_number\": issue_number,\n",
    "        \"supplement_index\": supplement_index,\n",
    "        \"is_supplement\": supplement_index > 0,\n",
    "        \"title\": title,\n",
    "        \"publication_key\": publication_key,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "\n",
    "\n",
    "def _normalize_doc_number(base: str, year: str | None) -> str:\n",
    "    for char in \"\\u2013\\u2014\\u2012\\u2212\":\n",
    "        base = base.replace(char, \"-\")\n",
    "    base = re.sub(r\"\\s+\", \"\", base.strip())\n",
    "    if year and year not in base:\n",
    "        base = f\"{base}/{year}\"\n",
    "    return base\n",
    "\n",
    "\n",
    "def _extract_doc_number(text: str) -> str | None:\n",
    "    for pattern in DOC_NUMBER_PATTERNS:\n",
    "        match = pattern.search(text)\n",
    "        if match:\n",
    "            base = match.group(1)\n",
    "            year = match.group(2) if len(match.groups()) >= 2 else None\n",
    "            return _normalize_doc_number(base, year)\n",
    "    return None\n",
    "\n",
    "\n",
    "def _determine_doc_type(header: str) -> str:\n",
    "    lower = header.lower()\n",
    "    mapping = [\n",
    "        (r\"ministerial decision\", \"Decision\"),\n",
    "        (r\"decree-law\", \"Decree-Law\"),\n",
    "        (r\"emiri decree\", \"Decree\"),\n",
    "        (r\"amiri decree\", \"Decree\"),\n",
    "        (r\"decree\", \"Decree\"),\n",
    "        (r\"resolution\", \"Resolution\"),\n",
    "        (r\"decision\", \"Decision\"),\n",
    "        (r\"memorandum\", \"Memorandum\"),\n",
    "        (r\"circular\", \"Circular\"),\n",
    "        (r\"tender\", \"Tender\"),\n",
    "        (r\"notice\", \"Notice\"),\n",
    "        (r\"invitation\", \"Invitation\"),\n",
    "        (r\"announcement\", \"Announcement\"),\n",
    "        (r\"law\", \"Law\"),\n",
    "        (r\"order\", \"Order\"),\n",
    "        (r\"statement\", \"Statement\"),\n",
    "        (r\"regulation\", \"Regulation\"),\n",
    "        (r\"contract\", \"Contract\"),\n",
    "        (r\"agreement\", \"Agreement\"),\n",
    "        (r\"award\", \"Award\"),\n",
    "        (r\"correction\", \"Correction\"),\n",
    "        (r\"report\", \"Report\")\n",
    "    ]\n",
    "    for pattern, doc_type in mapping:\n",
    "        if re.search(pattern, lower):\n",
    "            return doc_type\n",
    "    return \"Document\"\n",
    "\n",
    "\n",
    "def _detect_title(lines: list[str]) -> str | None:\n",
    "    for line in lines:\n",
    "        stripped = line.strip().lstrip(\"#\").strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "        lowered = stripped.lower()\n",
    "        if lowered.startswith(\"no.\"):\n",
    "            continue\n",
    "        if lowered.startswith(\"article\") or lowered.startswith(\"مادة\"):\n",
    "            continue\n",
    "        return stripped\n",
    "    return None\n",
    "\n",
    "\n",
    "def _detect_doc_start(line: str) -> bool:\n",
    "    stripped = line.lstrip()\n",
    "    hashes = len(stripped) - len(stripped.lstrip(\"#\"))\n",
    "    if hashes != 2:\n",
    "        return False\n",
    "    header = stripped.lstrip(\"#\").strip().lower()\n",
    "    return any(keyword in header for keyword in DOC_KEYWORDS)\n",
    "\n",
    "\n",
    "def iter_documents(text: str):\n",
    "    cleaned = BLOCK_RE.sub(\"\", text)\n",
    "    lines = cleaned.splitlines()\n",
    "    current_section = None\n",
    "    pending_pages: list[int] = []\n",
    "    current_doc = None\n",
    "\n",
    "    def flush(doc_state):\n",
    "        if not doc_state:\n",
    "            return None\n",
    "        doc_lines = doc_state[\"lines\"]\n",
    "        body = \"\\n\".join(doc_lines).strip()\n",
    "        if not body:\n",
    "            return None\n",
    "        header = doc_state[\"header\"]\n",
    "        doc_type = _determine_doc_type(header)\n",
    "        doc_number = _extract_doc_number(\"\\n\".join(doc_lines[:5]))\n",
    "        title = _detect_title(doc_lines[1:6])\n",
    "        page_numbers = sorted(set(doc_state.get(\"page_numbers\") or []))\n",
    "        return {\n",
    "            \"doc_type\": doc_type,\n",
    "            \"doc_number\": doc_number,\n",
    "            \"issuer\": None,\n",
    "            \"title\": title,\n",
    "            \"section_heading\": doc_state.get(\"section_heading\"),\n",
    "            \"page_start\": page_numbers[0] if page_numbers else None,\n",
    "            \"page_end\": page_numbers[-1] if page_numbers else None,\n",
    "            \"text\": body,\n",
    "        }\n",
    "\n",
    "    for raw_line in lines:\n",
    "        line = raw_line.rstrip()\n",
    "        if not line:\n",
    "            if current_doc:\n",
    "                current_doc[\"lines\"].append(\"\")\n",
    "            continue\n",
    "        page_start = PAGE_START_RE.search(line)\n",
    "        page_end = PAGE_END_RE.search(line)\n",
    "        if page_start:\n",
    "            pending_pages.append(int(page_start.group(1)))\n",
    "            continue\n",
    "        if page_end:\n",
    "            pending_pages.append(int(page_end.group(1)))\n",
    "            continue\n",
    "        stripped = line.lstrip()\n",
    "        hashes = len(stripped) - len(stripped.lstrip(\"#\"))\n",
    "        if hashes == 1:\n",
    "            current_section = stripped.lstrip(\"#\").strip()\n",
    "            continue\n",
    "        if _detect_doc_start(line):\n",
    "            doc_payload = flush(current_doc)\n",
    "            if doc_payload:\n",
    "                yield doc_payload\n",
    "            header = line.lstrip(\"#\").strip()\n",
    "            current_doc = {\n",
    "                \"header\": header,\n",
    "                \"lines\": [header],\n",
    "                \"section_heading\": current_section,\n",
    "                \"page_numbers\": list(pending_pages) if pending_pages else [],\n",
    "            }\n",
    "            pending_pages.clear()\n",
    "            continue\n",
    "        if current_doc is None:\n",
    "            continue\n",
    "        if pending_pages:\n",
    "            current_doc[\"page_numbers\"].extend(pending_pages)\n",
    "            pending_pages.clear()\n",
    "        current_doc[\"lines\"].append(line)\n",
    "\n",
    "    final_doc = flush(current_doc)\n",
    "    if final_doc:\n",
    "        yield final_doc\n",
    "\n",
    "\n",
    "\n",
    "def ensure_constraints(graph_client):\n",
    "    queries = [\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT publication_key_unique IF NOT EXISTS\n",
    "        FOR (p:Publication) REQUIRE p.publication_key IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT document_key_unique IF NOT EXISTS\n",
    "        FOR (d:Document) REQUIRE d.document_key IS UNIQUE\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS\n",
    "        FOR (c:Chunk) REQUIRE c.chunk_id IS UNIQUE\n",
    "        \"\"\",\n",
    "    ]\n",
    "    for query in queries:\n",
    "        graph_client.query(query)\n",
    "\n",
    "ensure_constraints(graph)\n",
    "\n",
    "graph.query(\n",
    "    \"\"\"\n",
    "    MERGE (g:Gazette {name: $name})\n",
    "    ON CREATE SET g.name = $name\n",
    "    \"\"\",\n",
    "    {\"name\": GAZETTE_NAME},\n",
    ")\n",
    "\n",
    "\n",
    "def _sanitize_doc_type(doc_type: str) -> str:\n",
    "    safe = re.sub(r\"\\s+\", \"-\", doc_type.strip())\n",
    "    return safe or \"Document\"\n",
    "\n",
    "md_files = sorted(SOURCE_DIR.rglob(\"*.md\"))\n",
    "if not md_files:\n",
    "    tqdm.write(\"No markdown files found for ingestion.\", file=sys.stderr)\n",
    "\n",
    "prepared_publications = []\n",
    "total_chunks = 0\n",
    "for path in md_files:\n",
    "    text = path.read_text(encoding=\"utf-8\")\n",
    "    publication = parse_publication_header(text, str(path))\n",
    "    publication[\"source_basename\"] = path.name\n",
    "    documents = []\n",
    "    doc_seq = 0\n",
    "    for doc in iter_documents(text):\n",
    "        doc_seq += 1\n",
    "        doc_type = doc[\"doc_type\"] or \"Document\"\n",
    "        key_doc_type = _sanitize_doc_type(doc_type)\n",
    "        doc_number = doc.get(\"doc_number\")\n",
    "        key_suffix = doc_number or f\"{doc_seq:03d}\"\n",
    "        document_key = f\"{publication['publication_key']}:{key_doc_type}:{key_suffix}\"\n",
    "        doc[\"document_key\"] = document_key\n",
    "        doc[\"doc_type\"] = doc_type\n",
    "        doc[\"doc_sequence\"] = doc_seq\n",
    "        documents.append(doc)\n",
    "    publication_context = {\n",
    "        \"publication_key\": publication[\"publication_key\"],\n",
    "        \"issue_number\": publication[\"issue_number\"],\n",
    "        \"supplement_index\": publication[\"supplement_index\"],\n",
    "        \"publication_date\": publication[\"publication_date\"],\n",
    "        \"source_basename\": publication[\"source_basename\"],\n",
    "    }\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_meta = {\n",
    "            \"document_key\": doc[\"document_key\"],\n",
    "            \"doc_type\": doc[\"doc_type\"],\n",
    "            \"doc_number\": doc.get(\"doc_number\"),\n",
    "            \"section_heading\": doc.get(\"section_heading\"),\n",
    "            \"page_start\": doc.get(\"page_start\"),\n",
    "            \"page_end\": doc.get(\"page_end\"),\n",
    "        }\n",
    "        doc_chunks = chunk_document_text(doc[\"text\"], publication_context, doc_meta, cfg=CHUNK_CONFIG)\n",
    "        doc[\"chunks\"] = doc_chunks\n",
    "        total_chunks += len(doc_chunks)\n",
    "        if doc.get(\"doc_sequence\") == 1 and doc_chunks:\n",
    "            preview_lines = doc_chunks[0][\"text\"].splitlines()[:4]\n",
    "            tqdm.write(\"Chunk preview: \" + \" | \".join(preview_lines), file=sys.stderr)\n",
    "    prepared_publications.append({\n",
    "        \"path\": path,\n",
    "        \"publication\": publication,\n",
    "        \"documents\": documents,\n",
    "    })\n",
    "    tqdm.write(\n",
    "        f\"Prepared {path.name}: {len(documents)} documents, {sum(len(d['chunks']) for d in documents)} chunks\",\n",
    "        file=sys.stderr,\n",
    "    )\n",
    "\n",
    "bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]'\n",
    "try:\n",
    "    term_width = shutil.get_terminal_size().columns\n",
    "except Exception:\n",
    "    term_width = 100\n",
    "\n",
    "\n",
    "def _ingest_publication_entry(entry, bar):\n",
    "    publication = entry[\"publication\"]\n",
    "    publication_props = {\n",
    "        \"slug\": publication[\"slug\"],\n",
    "        \"title\": publication.get(\"title\"),\n",
    "        \"publication_date\": publication[\"publication_date\"],\n",
    "        \"volume_number\": publication[\"volume_number\"],\n",
    "        \"issue_number\": publication[\"issue_number\"],\n",
    "        \"supplement_index\": publication[\"supplement_index\"],\n",
    "        \"is_supplement\": publication[\"is_supplement\"],\n",
    "        \"source_path\": str(entry[\"path\"]),\n",
    "    }\n",
    "\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE (g:Gazette {name: $gazette_name})\n",
    "        WITH g\n",
    "        MERGE (p:Publication {publication_key: $publication_key})\n",
    "        ON CREATE SET p += $props\n",
    "        ON MATCH SET p += $props\n",
    "        MERGE (g)-[:HAS_PUBLICATION]->(p)\n",
    "        \"\"\",\n",
    "        {\n",
    "            \"gazette_name\": GAZETTE_NAME,\n",
    "            \"publication_key\": publication[\"publication_key\"],\n",
    "            \"props\": publication_props,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def _ingest_one_doc(doc):\n",
    "        doc_props = {\n",
    "            \"doc_type\": doc[\"doc_type\"],\n",
    "            \"doc_number\": doc.get(\"doc_number\"),\n",
    "            \"issuer\": doc.get(\"issuer\"),\n",
    "            \"title\": doc.get(\"title\"),\n",
    "            \"section_heading\": doc.get(\"section_heading\"),\n",
    "            \"page_start\": doc.get(\"page_start\"),\n",
    "            \"page_end\": doc.get(\"page_end\"),\n",
    "            \"doc_sequence\": doc.get(\"doc_sequence\"),\n",
    "            \"publication_key\": publication[\"publication_key\"],\n",
    "        }\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            MATCH (p:Publication {publication_key: $publication_key})\n",
    "            MERGE (d:Document {document_key: $document_key})\n",
    "            ON CREATE SET d += $props\n",
    "            ON MATCH SET d += $props\n",
    "            MERGE (p)-[:CONTAINS]->(d)\n",
    "            \"\"\",\n",
    "            {\n",
    "                \"publication_key\": publication[\"publication_key\"],\n",
    "                \"document_key\": doc[\"document_key\"],\n",
    "                \"props\": doc_props,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        chunks = doc.get(\"chunks\") or []\n",
    "        if chunks:\n",
    "            graph.query(\n",
    "                \"\"\"\n",
    "                UNWIND $chunks AS chunk\n",
    "                MERGE (c:Chunk {chunk_id: chunk.chunk_id})\n",
    "                ON CREATE SET c += chunk\n",
    "                ON MATCH SET c += chunk\n",
    "                WITH c, chunk\n",
    "                MATCH (d:Document {document_key: chunk.document_key})\n",
    "                MERGE (d)-[:HAS_CHUNK]->(c)\n",
    "                \"\"\",\n",
    "                {\"chunks\": chunks},\n",
    "            )\n",
    "        return len(chunks)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futures = [ex.submit(_ingest_one_doc, d) for d in entry[\"documents\"]]\n",
    "        for fut in futures:\n",
    "            try:\n",
    "                count = fut.result()\n",
    "                if count:\n",
    "                    bar.update(count)\n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"Doc ingest error: {e}\", file=sys.stderr)\n",
    "\n",
    "start = time.perf_counter()\n",
    "with tqdm(\n",
    "    total=max(total_chunks, 1),\n",
    "    desc=f\"Ingesting chunks (workers={max_workers})\",\n",
    "    ncols=term_width,\n",
    "    bar_format=bar_format,\n",
    "    file=sys.stderr,\n",
    "    disable=(total_chunks == 0),\n",
    ") as bar:\n",
    "    for entry in prepared_publications:\n",
    "        _ingest_publication_entry(entry, bar)\n",
    "\n",
    "dt = time.perf_counter() - start\n",
    "if total_chunks == 0:\n",
    "    tqdm.write(\"No chunks were generated for ingestion.\", file=sys.stderr)\n",
    "else:\n",
    "    rate = total_chunks / max(dt, 1e-6)\n",
    "    tqdm.write(f\"Ingestion complete. Ingested {total_chunks} chunks in {dt:.1f}s ({rate:,.0f} chunks/s).\", file=sys.stderr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Core Counts ---\n",
      "Gazettes: 1\n",
      "Publications: 10 (expected 10)\n",
      "Documents: 567\n",
      "Chunks: 8488\n",
      "\n",
      "--- Supplement Spot Checks ---\n",
      "Issue 1736 -> 71-1736-0 (supplement=False, index=0)\n",
      "Issue 1736 -> 71-1736-2 (supplement=True, index=2)\n",
      "Issue 1738 -> 71-1738-0 (supplement=False, index=0)\n",
      "Issue 1738 -> 71-1738-1 (supplement=True, index=1)\n",
      "\n",
      "--- Document Key Samples ---\n",
      "71-1733-0 :: 71-1733-0:Announcement:uncement\n",
      "71-1733-0 :: 71-1733-0:Announcement:uncementArequesthasbeensubmittedtotheDepartmentofPartnerships\n",
      "71-1733-0 :: 71-1733-0:Announcement:uncementMessrs\n",
      "\n",
      "--- Chunk Header Preview ---\n",
      "Chunk 1: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] [SRC=2025-04-27_en.md] [DOC=Decision 2/2025] [SECTION=Public Authority for Manpower <orig>الهيئة العامة للقوى العاملة</orig>] [ARTICLE=Summary o\n",
      "Chunk 2: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] [SRC=2025-04-27_en.md] [DOC=Decision 2/2025] [SECTION=Public Authority for Manpower <orig>الهيئة العامة للقوى العاملة</orig>] [ARTICLE=Article T\n",
      "Chunk 3: [PUB=71-1736-0] [ISSUE=1736] [SUPP=0] [DATE=2025-04-27] [SRC=2025-04-27_en.md] [DOC=Decision 2/2025] [SECTION=Public Authority for Manpower <orig>الهيئة العامة للقوى العاملة</orig>] [ARTICLE=Article O\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Summary & validation checks\n",
    "# -------------------------------------------------------------\n",
    "from textwrap import dedent\n",
    "\n",
    "def run_query(query, params=None):\n",
    "    return graph.query(query, params or {})\n",
    "\n",
    "expected_publications = len(md_files) if 'md_files' in globals() else None\n",
    "\n",
    "print('\\n--- Core Counts ---')\n",
    "gazette_count = run_query('MATCH (g:Gazette) RETURN count(g) AS c')[0]['c']\n",
    "publication_count = run_query('MATCH (p:Publication) RETURN count(p) AS c')[0]['c']\n",
    "document_count = run_query('MATCH (d:Document) RETURN count(d) AS c')[0]['c']\n",
    "chunk_count = run_query('MATCH (c:Chunk) RETURN count(c) AS c')[0]['c']\n",
    "print(f'Gazettes: {gazette_count}')\n",
    "if expected_publications is not None:\n",
    "    print(f'Publications: {publication_count} (expected {expected_publications})')\n",
    "else:\n",
    "    print(f'Publications: {publication_count}')\n",
    "print(f'Documents: {document_count}')\n",
    "print(f'Chunks: {chunk_count}')\n",
    "\n",
    "print('\\n--- Supplement Spot Checks ---')\n",
    "for issue in (1736, 1738):\n",
    "    rows = run_query(\n",
    "        'MATCH (p:Publication {issue_number: $issue}) RETURN p.publication_key AS key, p.is_supplement AS is_supplement, p.supplement_index AS supplement_index ORDER BY p.supplement_index',\n",
    "        {'issue': issue},\n",
    "    )\n",
    "    if not rows:\n",
    "        print(f'Issue {issue}: no rows found')\n",
    "        continue\n",
    "    for row in rows:\n",
    "        print(f\"Issue {issue} -> {row['key']} (supplement={row['is_supplement']}, index={row['supplement_index']})\")\n",
    "\n",
    "print('\\n--- Document Key Samples ---')\n",
    "for row in run_query(\n",
    "    'MATCH (p:Publication)-[:CONTAINS]->(d:Document) RETURN p.publication_key AS pk, d.document_key AS dk ORDER BY d.document_key LIMIT 3'\n",
    "):\n",
    "    print(f\"{row['pk']} :: {row['dk']}\")\n",
    "\n",
    "print('\\n--- Chunk Header Preview ---')\n",
    "chunk_rows = run_query('MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk) RETURN c.text AS text LIMIT 3')\n",
    "if not chunk_rows:\n",
    "    print('No chunks found.')\n",
    "else:\n",
    "    for idx, row in enumerate(chunk_rows, start=1):\n",
    "        header_lines = row['text'].splitlines()[:2]\n",
    "        preview = ' | '.join(header_lines)\n",
    "        print(f'Chunk {idx}: {preview[:200]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (base:Publication {supplement_index: 0})\\nOPTIONAL MATCH (base)<-[:UPDATED_BY_PUBLICATION]-(sup:Publication)\\nWITH base, sup\\nORDER BY sup.publication_date DESC, sup.supplement_index DESC, sup.publication_key DESC\\nWITH base, [s IN collect(sup) WHERE s IS NOT NULL] AS supplements\\nSET base.latest_updated_by_publication_key =\\n  CASE\\n    WHEN size(supplements) = 0 THEN null\\n    ELSE supplements[0].publication_key\\n  END\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplement links: 2, bases with latest pointer: 2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Supplements + latest pointers + authority weights\n",
    "# -------------------------------------------------------------\n",
    "from textwrap import dedent\n",
    "\n",
    "def run(q, params=None):\n",
    "    return graph.query(q, params or {})\n",
    "\n",
    "# Link supplements to their base (same volume + issue; supplement_index>0)\n",
    "run(dedent('''\n",
    "MATCH (sup:Publication)\n",
    "WHERE sup.supplement_index > 0\n",
    "MATCH (base:Publication {volume_number: sup.volume_number, issue_number: sup.issue_number, supplement_index: 0})\n",
    "MERGE (sup)-[:UPDATED_BY_PUBLICATION]->(base)\n",
    "'''))\n",
    "\n",
    "# Remove legacy SUPPLEMENT_OF edges (if any remain)\n",
    "run('MATCH ()-[r:SUPPLEMENT_OF]->() DELETE r')\n",
    "\n",
    "# Set grouping key per publication\n",
    "run(dedent('''\n",
    "MATCH (p:Publication)\n",
    "SET p.issue_group_key = toString(p.volume_number) + '-' + toString(p.issue_number)\n",
    "'''))\n",
    "\n",
    "# Compute latest_updated_by_publication_key for base issues\n",
    "run(dedent('''\n",
    "MATCH (base:Publication {supplement_index: 0})\n",
    "OPTIONAL MATCH (base)<-[:UPDATED_BY_PUBLICATION]-(sup:Publication)\n",
    "WITH base, sup\n",
    "ORDER BY sup.publication_date DESC, sup.supplement_index DESC, sup.publication_key DESC\n",
    "WITH base, [s IN collect(sup) WHERE s IS NOT NULL] AS supplements\n",
    "SET base.latest_updated_by_publication_key =\n",
    "  CASE\n",
    "    WHEN size(supplements) = 0 THEN null\n",
    "    ELSE supplements[0].publication_key\n",
    "  END\n",
    "'''))\n",
    "\n",
    "# Clear pointer on supplements themselves\n",
    "run(dedent('''\n",
    "MATCH (p:Publication)\n",
    "WHERE p.supplement_index > 0\n",
    "SET p.latest_updated_by_publication_key = null\n",
    "'''))\n",
    "\n",
    "# Set authority weights by doc_type (simple heuristic)\n",
    "run(dedent('''\n",
    "MATCH (d:Document)\n",
    "SET d.authority_weight =\n",
    "  CASE d.doc_type\n",
    "    WHEN 'Decree-Law' THEN 1.30\n",
    "    WHEN 'Decree' THEN 1.20\n",
    "    WHEN 'Law' THEN 1.20\n",
    "    WHEN 'Decision' THEN 1.10\n",
    "    WHEN 'Resolution' THEN 1.05\n",
    "    WHEN 'Memorandum' THEN 1.00\n",
    "    WHEN 'Notice' THEN 0.95\n",
    "    WHEN 'Tender' THEN 0.90\n",
    "    ELSE 1.00\n",
    "  END\n",
    "'''))\n",
    "\n",
    "# Quick summary\n",
    "supp_links = run('MATCH (:Publication)-[:UPDATED_BY_PUBLICATION]->(:Publication) RETURN count(*) AS n')[0]['n']\n",
    "bases_with_latest = run('MATCH (p:Publication) WHERE p.supplement_index = 0 AND p.latest_updated_by_publication_key IS NOT NULL RETURN count(p) AS n')[0]['n']\n",
    "print(f\"Supplement links: {supp_links}, bases with latest pointer: {bases_with_latest}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n    MATCH (c:Chunk)\\n    RETURN count(c) AS total,\\n           count(c.closing_date) AS closing_dates,\\n           count(c.price_kd) AS price_values,\\n           count(c.guarantee_kd) AS guarantee_values\\n    '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized 8488 chunks (closing_date=632, price_kd=675, guarantee_kd=186).\n"
     ]
    }
   ],
   "source": [
    "# --- Normalize chunk text and extract procurement facts ---\n",
    "from KnowledgeGraph.normalize import normalize_chunk\n",
    "\n",
    "rows = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    RETURN c.chunk_id AS chunk_id, c.text AS text\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "updates = []\n",
    "for row in rows:\n",
    "    normalized = normalize_chunk(row[\"text\"] or \"\")\n",
    "    facts = normalized.get(\"facts\") or {}\n",
    "    updates.append(\n",
    "        {\n",
    "            \"chunk_id\": row[\"chunk_id\"],\n",
    "            \"text_norm\": normalized.get(\"text_norm\"),\n",
    "            \"table_kv\": normalized.get(\"table_kv\"),\n",
    "            \"closing_date\": facts.get(\"closing_date\"),\n",
    "            \"price_kd\": facts.get(\"price_kd\"),\n",
    "            \"guarantee_kd\": facts.get(\"guarantee_kd\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "if updates:\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        UNWIND $rows AS row\n",
    "        MATCH (c:Chunk {chunk_id: row.chunk_id})\n",
    "        SET c.text_norm = row.text_norm,\n",
    "            c.table_kv = row.table_kv,\n",
    "            c.closing_date = row.closing_date,\n",
    "            c.price_kd = row.price_kd,\n",
    "            c.guarantee_kd = row.guarantee_kd\n",
    "        \"\"\",\n",
    "        {\"rows\": updates},\n",
    "    )\n",
    "\n",
    "summary = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (c:Chunk)\n",
    "    RETURN count(c) AS total,\n",
    "           count(c.closing_date) AS closing_dates,\n",
    "           count(c.price_kd) AS price_values,\n",
    "           count(c.guarantee_kd) AS guarantee_values\n",
    "    \"\"\"\n",
    ")[0]\n",
    "print(\n",
    "    \"Normalized {total} chunks (closing_date={closing_dates}, price_kd={price_values}, guarantee_kd={guarantee_values}).\".format(\n",
    "        **summary\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (c:Chunk)\\nRETURN count(c) AS total,\\n       count(c.page_start) AS with_start,\\n       count(c.page_end) AS with_end\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk source stamped; page coverage -> start: 7462/8488, end: 7462/8488\n"
     ]
    }
   ],
   "source": [
    "# --- Stamp chunk source (publication_key) and ensure page bounds ---\n",
    "from textwrap import dedent\n",
    "\n",
    "run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        MATCH (d)-[:PUBLISHED_IN]->(p:Publication)\n",
    "        SET c.source = p.publication_key\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "page_stats = run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (c:Chunk)\n",
    "        RETURN count(c) AS total,\n",
    "               count(c.page_start) AS with_start,\n",
    "               count(c.page_end) AS with_end\n",
    "        \"\"\"\n",
    "    )\n",
    ")[0]\n",
    "print(\n",
    "    \"Chunk source stamped; page coverage -> start: {with_start}/{total}, end: {with_end}/{total}\".format(\n",
    "        **page_stats\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest all data and create index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\n    MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\\n    RETURN d.document_key AS dk,\\n           d.doc_type AS doc_type,\\n           collect(c.text) AS texts,\\n           collect(c.table_kv) AS tables\\n    '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Anchor extraction + indexes (persistent) ---\n",
    "import re\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "_ARABIC_DIGITS = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "\n",
    "def _norm_digits(s: str) -> str:\n",
    "    return (s or \"\").translate(_ARABIC_DIGITS)\n",
    "\n",
    "def _normalize_anchor_value(anchor: str) -> str:\n",
    "    t = _norm_digits(anchor or \"\")\n",
    "    t = re.sub(r\"[\\u200f\\u200e\\u00a0\\u202f]\", \" \", t)\n",
    "    t = t.translate(str.maketrans({\"‐\": \"-\", \"‑\": \"-\", \"‒\": \"-\", \"–\": \"-\", \"—\": \"-\", \"−\": \"-\"}))\n",
    "    t = t.strip()\n",
    "    if not t:\n",
    "        return \"\"\n",
    "    t = t.upper()\n",
    "    t = re.sub(r\"(?<=\\b[A-Z])\\.(?=[A-Z])\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"\\s*([:/-])\\s*\", r\"\\1\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "_ANCHOR_KIND_RULES: List[tuple] = [\n",
    "    (re.compile(r\"\\bRFP\\b\"), \"tender\"),\n",
    "    (re.compile(r\"\\bTENDER\\b\"), \"tender\"),\n",
    "    (re.compile(r\"\\bHRA\\b\"), \"tender\"),\n",
    "    (re.compile(r\"\\bPRACTICE\\b\"), \"tender\"),\n",
    "    (re.compile(r\"\\bDECREE\\b\"), \"decision\"),\n",
    "    (re.compile(r\"\\bDECISION\\b\"), \"decision\"),\n",
    "    (re.compile(r\"\\bLAW\\b\"), \"decision\"),\n",
    "    (re.compile(r\"\\bA/M/\\d+\\b\"), \"case\"),\n",
    "    (re.compile(r\"\\b5D[A-Z0-9]+\\b\"), \"contract\"),\n",
    "    (re.compile(r\"\\bCERTIFICAT\"), \"case\"),\n",
    "    (re.compile(r\"\\bMEETING\\b\"), \"decision\"),\n",
    "]\n",
    "\n",
    "_TENDER_DOC_TYPES = {\"tender\", \"notice\", \"announcement\"}\n",
    "\n",
    "_RX_ANCHORS = [\n",
    "    re.compile(r\"\\bRFP[-\\s]*\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b\\d{4,7}[-\\s]*RFP\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bH\\.?\\s*R\\.?\\s*A\\.?\\s*\\d+\\s*/\\s*\\d+\\s*/\\s*\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bTENDER\\s+NO\\.?\\s*(?:[:\\-])?\\s*\\d+\\s*/\\s*\\d+\\s*/\\s*\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bPRACTICE\\s+NO\\.?\\s*[A-Z0-9]+(?:\\s*[-/]\\s*[A-Z0-9]+)+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bA/M/\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b5D[A-Z0-9]+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b\\d{4}/\\d{5}\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\bMEETING (?:MINUTES )?NO\\.?\\s*\\d{4}/\\d+\\b\", re.IGNORECASE),\n",
    "    re.compile(r\"\\b(?:DECREE-?LAW|LAW)\\s*(?:NO\\.?\\s*)?\\d+/?\\d{4}\\b\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def extract_anchors_from_text(text: str) -> list[str]:\n",
    "    t = _norm_digits(text or \"\")\n",
    "    out: list[str] = []\n",
    "    for rx in _RX_ANCHORS:\n",
    "        out.extend(rx.findall(t))\n",
    "    seen = set(); uniq: list[str] = []\n",
    "    for a in out:\n",
    "        k = a.upper()\n",
    "        if k not in seen:\n",
    "            seen.add(k); uniq.append(a)\n",
    "    return uniq\n",
    "\n",
    "def _classify_anchor(value: str) -> str:\n",
    "    for rx, kind in _ANCHOR_KIND_RULES:\n",
    "        if rx.search(value):\n",
    "            return kind\n",
    "    if re.search(r\"\\d{4}/\\d{5}\", value):\n",
    "        return \"case\"\n",
    "    if value.startswith(\"EPA/\") or value.startswith(\"CAPT/\"):\n",
    "        return \"tender\"\n",
    "    if value.startswith(\"HRA\") or value.startswith(\"TENDER\"):\n",
    "        return \"tender\"\n",
    "    if re.match(r\"^[A-Z]*\\d+/\\d+/\\d+$\", value):\n",
    "        return \"tender\"\n",
    "    return \"other\"\n",
    "\n",
    "def _build_anchor_records(texts: List[str]) -> List[Dict[str, str]]:\n",
    "    records: List[Dict[str, str]] = []\n",
    "    seen: Set[str] = set()\n",
    "    for text in texts:\n",
    "        for raw in extract_anchors_from_text(text):\n",
    "            value = _normalize_anchor_value(raw)\n",
    "            if not value or value in seen:\n",
    "                continue\n",
    "            seen.add(value)\n",
    "            records.append(\n",
    "                {\n",
    "                    \"kind\": _classify_anchor(value),\n",
    "                    \"value\": value,\n",
    "                    \"raw\": raw.strip(),\n",
    "                }\n",
    "            )\n",
    "    return records\n",
    "\n",
    "\n",
    "def _filter_anchor_records(records: List[Dict[str, str]], doc_type: str | None) -> List[Dict[str, str]]:\n",
    "    if not records:\n",
    "        return records\n",
    "    doc_type_norm = (doc_type or \"\").strip().lower()\n",
    "    if doc_type_norm in _TENDER_DOC_TYPES:\n",
    "        return records\n",
    "    return [rec for rec in records if rec.get(\"kind\") != \"tender\"]\n",
    "\n",
    "\n",
    "run = (lambda q, p=None: graph.query(q, p or {}))\n",
    "run(\"CREATE CONSTRAINT anchor_unique IF NOT EXISTS FOR (a:Anchor) REQUIRE (a.kind, a.value) IS UNIQUE\")\n",
    "run(\"CREATE CONSTRAINT doc_key_unique IF NOT EXISTS FOR (d:Document) REQUIRE d.document_key IS UNIQUE\")\n",
    "run(\"CREATE INDEX doc_status IF NOT EXISTS FOR (d:Document) ON (d.status)\")\n",
    "\n",
    "# Compute anchors per Document from its chunks and persist\n",
    "rows = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "    RETURN d.document_key AS dk,\n",
    "           d.doc_type AS doc_type,\n",
    "           collect(c.text) AS texts,\n",
    "           collect(c.table_kv) AS tables\n",
    "    \"\"\"\n",
    ")\n",
    "for row in rows:\n",
    "    texts: List[str] = [t for t in (row.get(\"texts\") or []) if t]\n",
    "    tables: List[str] = [t for t in (row.get(\"tables\") or []) if t]\n",
    "    doc_type = row.get(\"doc_type\")\n",
    "    anchor_records = _filter_anchor_records(\n",
    "        _build_anchor_records(texts + tables),\n",
    "        doc_type,\n",
    "    )\n",
    "    anchor_values = [rec[\"value\"] for rec in anchor_records]\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document {document_key: $dk})\n",
    "        SET d.anchors = $anchors\n",
    "        \"\"\",\n",
    "        {\"dk\": row[\"dk\"], \"anchors\": anchor_values},\n",
    "    )\n",
    "    if anchor_records:\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            UNWIND $anchor_rows AS anchor\n",
    "            MERGE (a:Anchor {kind: anchor.kind, value: anchor.value})\n",
    "            ON CREATE SET a.display_value = anchor.raw\n",
    "            ON MATCH SET a.display_value = coalesce(a.display_value, anchor.raw)\n",
    "            \"\"\",\n",
    "            {\"anchor_rows\": anchor_records},\n",
    "        )\n",
    "\n",
    "# Helpful fulltext/property indexes (idempotent)\n",
    "run(\"CREATE FULLTEXT INDEX chunk_text_idx IF NOT EXISTS FOR (c:Chunk) ON EACH [c.text]\")\n",
    "run(\"CREATE FULLTEXT INDEX doc_text_idx   IF NOT EXISTS FOR (d:Document) ON EACH [d.title, d.section_heading, d.doc_number, d.document_key]\")\n",
    "run(\"CREATE FULLTEXT INDEX doc_anchor_ft IF NOT EXISTS FOR (d:Document) ON EACH [d.title, d.section_heading, d.doc_number, d.document_key, d.anchors]\")\n",
    "run(\"CREATE FULLTEXT INDEX doc_anchors_idx IF NOT EXISTS FOR (d:Document) ON EACH [d.anchors]\")\n",
    "run(\"CREATE INDEX doc_pub_key_idx IF NOT EXISTS FOR (d:Document) ON (d.publication_key)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Rebuild HAS_ANCHOR relationships from d.anchors ---\n",
    "run(\n",
    "    \"\"\"\n",
    "    MATCH (d:Document)\n",
    "    OPTIONAL MATCH (d)-[r:HAS_ANCHOR]->(:Anchor)\n",
    "    DELETE r\n",
    "    WITH d\n",
    "    UNWIND coalesce(d.anchors, []) AS value\n",
    "    MATCH (a:Anchor {value: value})\n",
    "    MERGE (d)-[:HAS_ANCHOR]->(a)\n",
    "    \"\"\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: '\\nMATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\\nWITH d, min(c.page_start) AS minStart, max(c.page_end) AS maxEnd\\nSET d.page_start = coalesce(d.page_start, minStart),\\n    d.page_end   = coalesce(d.page_end,   maxEnd)\\n'\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.AggregationSkippedNull} {category: UNRECOGNIZED} {title: The query contains an aggregation function that skips null values.} {description: null value eliminated in set function.} {position: None} for query: 'MATCH (c:Chunk) RETURN count(c) AS total, count(c.page_start) AS withStart, count(c.page_end) AS withEnd'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUBLISHED_IN: pubs without docs=0, docs missing edge=0\n",
      "Chunk pages: total=8488, withStart=7866, withEnd=7866\n"
     ]
    }
   ],
   "source": [
    "# --- Create PUBLISHED_IN edges + Publication->LATEST and page-number backfills ---\n",
    "from textwrap import dedent\n",
    "\n",
    "run = (lambda q, p=None: graph.query(q, p or {}))\n",
    "\n",
    "# 1) Create PUBLISHED_IN edges by publication_key\n",
    "run(dedent('''\n",
    "MATCH (d:Document)\n",
    "WHERE d.publication_key IS NOT NULL AND NOT (d)-[:PUBLISHED_IN]->(:Publication)\n",
    "MATCH (p:Publication {publication_key: d.publication_key})\n",
    "MERGE (d)-[:PUBLISHED_IN]->(p)\n",
    "'''))\n",
    "\n",
    "# 2) One LATEST per Publication to newest Document (by doc_sequence desc)\n",
    "run(dedent('''\n",
    "MATCH (p:Publication)-[:CONTAINS]->(d:Document)\n",
    "WITH p, d ORDER BY d.doc_sequence DESC, d.document_key DESC\n",
    "WITH p, collect(d)[0] AS latest\n",
    "MERGE (p)-[:LATEST]->(latest)\n",
    "WITH p\n",
    "MATCH (p)-[r:LATEST]->(:Document)\n",
    "WITH p, collect(r) AS rels\n",
    "FOREACH (r IN rels[1..] | DELETE r)\n",
    "'''))\n",
    "\n",
    "# 3) Backfill Document page_start/page_end from chunks where missing\n",
    "run(dedent('''\n",
    "MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "WITH d, min(c.page_start) AS minStart, max(c.page_end) AS maxEnd\n",
    "SET d.page_start = coalesce(d.page_start, minStart),\n",
    "    d.page_end   = coalesce(d.page_end,   maxEnd)\n",
    "'''))\n",
    "\n",
    "# 4) Propagate doc page numbers to chunks where missing\n",
    "run(dedent('''\n",
    "MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "SET c.page_start = coalesce(c.page_start, d.page_start),\n",
    "    c.page_end   = coalesce(c.page_end,   d.page_end)\n",
    "'''))\n",
    "\n",
    "# 5) Quick summary\n",
    "pubs_without_docs = run('MATCH (p:Publication) WHERE NOT (:Document)-[:PUBLISHED_IN]->(p) RETURN count(p) AS n')[0]['n']\n",
    "docs_missing_edge = run('MATCH (d:Document) WHERE NOT (d)-[:PUBLISHED_IN]->(:Publication) RETURN count(d) AS n')[0]['n']\n",
    "chunk_counts = run('MATCH (c:Chunk) RETURN count(c) AS total, count(c.page_start) AS withStart, count(c.page_end) AS withEnd')[0]\n",
    "print(f\"PUBLISHED_IN: pubs without docs={pubs_without_docs}, docs missing edge={docs_missing_edge}\")\n",
    "print(f\"Chunk pages: total={chunk_counts['total']}, withStart={chunk_counts['withStart']}, withEnd={chunk_counts['withEnd']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Propagate anchors across document updates and supplements ---\n",
    "# (Handled in the combined anchor refresh cell below.)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Anchor LATEST rebuild (after PUBLISHED_IN edges) ---\n",
    "# (Handled in the combined anchor refresh cell below.)\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document status summary: {'announced': 500, 'canceled': 8, 'corrected': 21, 'extended': 17, 'awarded': 18, 'revoked': 3}\n"
     ]
    }
   ],
   "source": [
    "# --- Document update detection (corrections / cancellations / etc.) ---\n",
    "_UPDATE_PATTERNS = [\n",
    "    (re.compile(r\"\\bcorrection(s)?\\b\", re.IGNORECASE), (\"correction\", \"corrected\")),\n",
    "    (re.compile(r\"\\bamend(ed|ment|ments)\\b\", re.IGNORECASE), (\"correction\", \"corrected\")),\n",
    "    (re.compile(r\"استدراك\"), (\"correction\", \"corrected\")),\n",
    "    (re.compile(r\"تصحيح\"), (\"correction\", \"corrected\")),\n",
    "    (re.compile(r\"\\bcancell?ation\\b\", re.IGNORECASE), (\"cancellation\", \"canceled\")),\n",
    "    (re.compile(r\"\\bcancel(l)?ed\\b\", re.IGNORECASE), (\"cancellation\", \"canceled\")),\n",
    "    (re.compile(r\"إلغاء\"), (\"cancellation\", \"canceled\")),\n",
    "    (re.compile(r\"يلغى\"), (\"cancellation\", \"canceled\")),\n",
    "    (re.compile(r\"\\bwithdraw(al|n)?\\b\", re.IGNORECASE), (\"revocation\", \"revoked\")),\n",
    "    (re.compile(r\"سحب\"), (\"revocation\", \"revoked\")),\n",
    "    (re.compile(r\"\\brevocation\\b\", re.IGNORECASE), (\"revocation\", \"revoked\")),\n",
    "    (re.compile(r\"\\bextend(ed|s|ing)?\\b\", re.IGNORECASE), (\"extension\", \"extended\")),\n",
    "    (re.compile(r\"\\bextension\\b\", re.IGNORECASE), (\"extension\", \"extended\")),\n",
    "    (re.compile(r\"تمديد\"), (\"extension\", \"extended\")),\n",
    "    (re.compile(r\"\\bpostpone(d|ment)?\\b\", re.IGNORECASE), (\"postponement\", \"postponed\")),\n",
    "    (re.compile(r\"تأجيل\"), (\"postponement\", \"postponed\")),\n",
    "    (re.compile(r\"\\baward(ed|ing)?\\b\", re.IGNORECASE), (\"award\", \"awarded\")),\n",
    "    (re.compile(r\"ترسية\"), (\"award\", \"awarded\")),\n",
    "]\n",
    "\n",
    "def _detect_update_status(text: str):\n",
    "    normalized = _norm_digits(text or \"\")\n",
    "    for pattern, payload in _UPDATE_PATTERNS:\n",
    "        if pattern.search(normalized):\n",
    "            return payload\n",
    "    return None\n",
    "\n",
    "run(\"MATCH ()-[r:UPDATES]->() DELETE r\")\n",
    "run(\"MATCH (d:Document) SET d.status = 'announced'\")\n",
    "\n",
    "doc_rows = graph.query(\n",
    "    \"\"\"\n",
    "    MATCH (d:Document)-[:PUBLISHED_IN]->(p:Publication)\n",
    "    RETURN d.document_key AS key,\n",
    "           coalesce(d.title, '') AS title,\n",
    "           coalesce(d.section_heading, '') AS section,\n",
    "           coalesce(d.doc_type, '') AS doc_type,\n",
    "           coalesce(d.anchors, []) AS anchors,\n",
    "           p.publication_date AS publication_date\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "for row in doc_rows:\n",
    "    context_parts = [row[\"title\"], row[\"section\"], row[\"doc_type\"]]\n",
    "    context_text = \" \".join(part for part in context_parts if part)\n",
    "    detected = _detect_update_status(context_text)\n",
    "    if not detected:\n",
    "        continue\n",
    "    kind, status = detected\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document {document_key: $doc_key})\n",
    "        SET d.status = $status\n",
    "        \"\"\",\n",
    "        {\"doc_key\": row[\"key\"], \"status\": status},\n",
    "    )\n",
    "    anchors = [a for a in (row[\"anchors\"] or []) if a]\n",
    "    if not anchors:\n",
    "        continue\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MATCH (d_new:Document {document_key: $doc_key})-[:PUBLISHED_IN]->(p_new:Publication)\n",
    "        UNWIND $anchors AS anchor_value\n",
    "        MATCH (a:Anchor {value: anchor_value})<-[:HAS_ANCHOR]-(d_old:Document)\n",
    "        MATCH (d_old)-[:PUBLISHED_IN]->(p_old:Publication)\n",
    "        WHERE d_old <> d_new AND p_old.publication_date <= p_new.publication_date\n",
    "        WITH DISTINCT d_new, d_old\n",
    "        MERGE (d_new)-[:UPDATES {kind: $kind}]->(d_old)\n",
    "        \"\"\",\n",
    "        {\"doc_key\": row[\"key\"], \"anchors\": anchors, \"kind\": kind},\n",
    "    )\n",
    "\n",
    "status_summary = {\n",
    "    row[\"status\"] or \"unknown\": row[\"count\"]\n",
    "    for row in graph.query(\"MATCH (d:Document) RETURN d.status AS status, count(*) AS count\")\n",
    "}\n",
    "print(\"Document status summary:\", status_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-04 :: 71-1737-0:Announcement:uncementTheInternalProcurementCommittee \n",
      "2025-04-27 :: 71-1736-0:Notice:uncement/NoticeRegardingtheAdditionofCompaniestoTenderNo \n"
     ]
    }
   ],
   "source": [
    "# --- Optional: verify anchor freshness for a specific identifier ---\n",
    "anchor_value = \"RFP-2113556\"\n",
    "check_rows = run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        WITH $anchor_value AS val\n",
    "        MATCH (a:Anchor {value: val})<-[:HAS_ANCHOR]-(d:Document)-[:PUBLISHED_IN]->(p:Publication)\n",
    "        OPTIONAL MATCH (a)-[:LATEST]->(cur:Document)\n",
    "        RETURN p.publication_date AS pub_date,\n",
    "               d.document_key AS doc,\n",
    "               d = cur AS is_latest\n",
    "        ORDER BY pub_date DESC, doc DESC\n",
    "        \"\"\"\n",
    "    ),\n",
    "    {\"anchor_value\": anchor_value},\n",
    ")\n",
    "if not check_rows:\n",
    "    print(f\"Anchor {anchor_value} not found (no documents)\")\n",
    "else:\n",
    "    for row in check_rows:\n",
    "        flag = \"<-- LATEST\" if row[\"is_latest\"] else \"\"\n",
    "        print(f\"{row['pub_date']} :: {row['doc']} {flag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superseded counts: {False: 567} | anchor latest links: 0\n"
     ]
    }
   ],
   "source": [
    "# --- Anchor freshness summary ---\n",
    "superseded_counts = {\n",
    "    row[\"flag\"]: row[\"count\"]\n",
    "    for row in run('MATCH (d:Document) RETURN coalesce(d.superseded,false) AS flag, count(*) AS count')\n",
    "}\n",
    "latest_links = run('MATCH (:Anchor)-[:LATEST]->(:Document) RETURN count(*) AS n')[0]['n']\n",
    "print(\"Superseded counts:\", superseded_counts, \"| anchor latest links:\", latest_links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors before propagation: 404 linked to 115 documents.\n",
      "Anchors after propagation: 404 linked to 115 documents.\n",
      "Anchor LATEST edges rebuilt: 404\n",
      "2025-05-04 :: 71-1737-0:Announcement:uncementTheInternalProcurementCommittee <-- LATEST\n",
      "2025-04-27 :: 71-1736-0:Notice:uncement/NoticeRegardingtheAdditionofCompaniestoTenderNo \n"
     ]
    }
   ],
   "source": [
    "# --- Propagate anchors and rebuild LATEST ---\n",
    "from textwrap import dedent\n",
    "\n",
    "_ALLOWED_TENDER_DOC_TYPES = [t.lower() for t in _TENDER_DOC_TYPES]\n",
    "\n",
    "def _run(query: str, params: dict | None = None):\n",
    "    return graph.query(query, params or {})\n",
    "\n",
    "before_stats = _run(\n",
    "    \"\"\"\n",
    "    MATCH (a:Anchor)<-[:HAS_ANCHOR]-(d:Document)\n",
    "    RETURN count(DISTINCT a) AS anchors_with_docs,\n",
    "           count(DISTINCT d) AS documents_with_anchors\n",
    "    \"\"\"\n",
    ")[0]\n",
    "print(\n",
    "    \"Anchors before propagation: {anchors_with_docs} linked to {documents_with_anchors} documents.\".format(\n",
    "        **before_stats\n",
    "    )\n",
    ")\n",
    "\n",
    "_run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (new:Document)-[:UPDATES]->(old:Document)-[:HAS_ANCHOR]->(a:Anchor)\n",
    "        WHERE (a.kind <> 'tender' OR toLower(coalesce(new.doc_type, '')) IN $allowed)\n",
    "          AND a.value IN coalesce(new.anchors, [])\n",
    "        MERGE (new)-[:HAS_ANCHOR]->(a)\n",
    "        \"\"\"\n",
    "    ),\n",
    "    {\"allowed\": _ALLOWED_TENDER_DOC_TYPES},\n",
    ")\n",
    "\n",
    "_run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (latest:Publication)<-[:PUBLISHED_IN]-(latest_doc:Document)\n",
    "        MATCH (latest)-[:UPDATED_BY_PUBLICATION]->(base:Publication)\n",
    "        MATCH (base)<-[:PUBLISHED_IN]-(base_doc:Document)-[:HAS_ANCHOR]->(a:Anchor)\n",
    "        WHERE (a.kind <> 'tender' OR toLower(coalesce(latest_doc.doc_type, '')) IN $allowed)\n",
    "          AND a.value IN coalesce(latest_doc.anchors, [])\n",
    "        MERGE (latest_doc)-[:HAS_ANCHOR]->(a)\n",
    "        \"\"\"\n",
    "    ),\n",
    "    {\"allowed\": _ALLOWED_TENDER_DOC_TYPES},\n",
    ")\n",
    "\n",
    "_run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document)-[r:HAS_ANCHOR]->(a:Anchor)\n",
    "        WHERE a.kind = 'tender' AND NOT toLower(coalesce(d.doc_type, '')) IN $allowed\n",
    "        DELETE r\n",
    "        \"\"\"\n",
    "    ),\n",
    "    {\"allowed\": _ALLOWED_TENDER_DOC_TYPES},\n",
    ")\n",
    "\n",
    "_run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document)-[:PUBLISHED_IN]->(p:Publication)\n",
    "        SET d.superseded = false,\n",
    "            d.effective_date = coalesce(d.effective_date, p.publication_date)\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "_run(\"MATCH (:Anchor)-[r:LATEST]->(:Document) DELETE r\")\n",
    "\n",
    "_run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (a:Anchor)<-[:HAS_ANCHOR]-(d:Document)-[:PUBLISHED_IN]->(p:Publication)\n",
    "        WITH a, d, p\n",
    "        ORDER BY p.publication_date DESC,\n",
    "                 coalesce(p.supplement_index, 0) DESC,\n",
    "                 d.document_key DESC\n",
    "        WITH a, collect(d) AS docs\n",
    "        WITH a, docs WHERE size(docs) > 0\n",
    "        UNWIND range(0, size(docs) - 1) AS idx\n",
    "        WITH a, docs, idx, docs[idx] AS doc\n",
    "        SET doc.superseded = (idx > 0)\n",
    "        WITH DISTINCT a, docs[0] AS newest\n",
    "        MERGE (a)-[:LATEST]->(newest)\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "_run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk)\n",
    "        SET c.superseded = coalesce(d.superseded, false)\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "after_stats = _run(\n",
    "    \"\"\"\n",
    "    MATCH (a:Anchor)<-[:HAS_ANCHOR]-(d:Document)\n",
    "    RETURN count(DISTINCT a) AS anchors_with_docs,\n",
    "           count(DISTINCT d) AS documents_with_anchors\n",
    "    \"\"\"\n",
    ")[0]\n",
    "print(\n",
    "    \"Anchors after propagation: {anchors_with_docs} linked to {documents_with_anchors} documents.\".format(\n",
    "        **after_stats\n",
    "    )\n",
    ")\n",
    "\n",
    "latest_count = _run(\"MATCH (:Anchor)-[:LATEST]->(:Document) RETURN count(*) AS n\")[0][\"n\"]\n",
    "print(f\"Anchor LATEST edges rebuilt: {latest_count}\")\n",
    "\n",
    "sample_rows = _run(\n",
    "    dedent(\n",
    "        \"\"\"\n",
    "        WITH 'RFP-2113556' AS val\n",
    "        MATCH (a:Anchor {value: val})<-[:HAS_ANCHOR]-(d:Document)-[:PUBLISHED_IN]->(p:Publication)\n",
    "        OPTIONAL MATCH (a)-[:LATEST]->(cur:Document)\n",
    "        RETURN p.publication_date AS pub_date,\n",
    "               d.document_key AS doc,\n",
    "               d = cur AS is_latest\n",
    "        ORDER BY pub_date DESC, doc DESC\n",
    "        \"\"\"\n",
    "    )\n",
    ")\n",
    "if not sample_rows:\n",
    "    print(\"Anchor RFP-2113556 not found (no documents)\")\n",
    "else:\n",
    "    for row in sample_rows:\n",
    "        flag = \"<-- LATEST\" if row[\"is_latest\"] else \"\"\n",
    "        print(f\"{row['pub_date']} :: {row['doc']} {flag}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding update...\n",
      "Found 8488 nodes without embeddings.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding nodes: 100%|██████████████████████████████████████████| 8488/8488 [01:28<00:00, 96.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished embedding update.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Vector Index\n",
    "create_vector_index(graph=graph, index_name='Chunk')\n",
    "\n",
    "# Embed Chunks\n",
    "embed_text(graph=graph, OPENAI_API_KEY=openAI_api, OPENAI_ENDPOINT=openAI_endpoint, node_name='Chunk', model_name=openAI_model, max_workers=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphrag-developer-challenge3-4suqu0LP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
