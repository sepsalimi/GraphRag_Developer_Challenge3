{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GraphRAG Prep (neo4j-graphrag)\n",
        "\n",
        "This notebook mirrors the original prep workflow but relies on the official `neo4j-graphrag` utilities for vector indexing, embedding, and optional retrieval checks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "print(sys.executable)\n",
        "print(\"Kernel ready.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Load project configuration and verify required Neo4j credentials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ImportError:\n",
        "    load_dotenv = None\n",
        "\n",
        "PROJECT_ROOT = Path(\"..\" ).resolve()\n",
        "ENV_PATH = PROJECT_ROOT / \".env\"\n",
        "if load_dotenv and ENV_PATH.exists():\n",
        "    load_dotenv(ENV_PATH)\n",
        "\n",
        "NEO4J_URI = os.getenv(\"NEO4J_URI\")\n",
        "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\n",
        "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\n",
        "\n",
        "missing = [name for name, value in [\n",
        "    (\"NEO4J_URI\", NEO4J_URI),\n",
        "    (\"NEO4J_USERNAME\", NEO4J_USERNAME),\n",
        "    (\"NEO4J_PASSWORD\", NEO4J_PASSWORD),\n",
        "] if not value]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing required environment variables: {', '.join(missing)}\")\n",
        "\n",
        "print(\"Loaded Neo4j credentials from environment.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to Neo4j\n",
        "\n",
        "Initialize a shared Neo4j driver that downstream steps reuse.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\") or None\n",
        "\n",
        "driver = GraphDatabase.driver(\n",
        "    NEO4J_URI,\n",
        "    auth=(NEO4J_USERNAME, NEO4J_PASSWORD),\n",
        ")\n",
        "\n",
        "print(\"Driver initialized.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with driver.session(database=NEO4J_DATABASE) as session:\n",
        "    result = session.run(\"RETURN 1 AS ok\")\n",
        "    print(result.single()[\"ok\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional Reset\n",
        "\n",
        "Use this only when you need a clean workspace. Comment the `session.run` lines you do not require.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment the lines below if you want to wipe previously generated lexical graph data.\n",
        "# with driver.session(database=NEO4J_DATABASE) as session:\n",
        "#     session.run(\"MATCH (c:Chunk) DETACH DELETE c\")\n",
        "#     session.run(\"MATCH (d:Document) DETACH DELETE d\")\n",
        "#     session.run(\"MATCH (n:__Entity__) DETACH DELETE n\")\n",
        "#     session.run(\"MATCH ()-[r:FROM_DOCUMENT|:NEXT_CHUNK|:FROM_CHUNK]->() DELETE r\")\n",
        "#     print(\"Cleared lexical graph nodes and relationships.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Neo4j GraphRAG Pipeline\n",
        "\n",
        "The official `SimpleKGPipeline` orchestrates chunking, entity extraction, and embedding. Adjust the schema or model choices as needed for your corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from neo4j_graphrag.embeddings import OpenAIEmbeddings\n",
        "from neo4j_graphrag.experimental.pipeline.kg_builder import SimpleKGPipeline\n",
        "from neo4j_graphrag.llm.openai_llm import OpenAILLM\n",
        "\n",
        "EMBED_MODEL = os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\n",
        "LLM_MODEL = os.getenv(\"OPENAI_LLM_MODEL\", \"gpt-4o\")\n",
        "LLM_MAX_TOKENS = int(os.getenv(\"OPENAI_LLM_MAX_TOKENS\", \"2000\"))\n",
        "LLM_TEMPERATURE = float(os.getenv(\"OPENAI_LLM_TEMPERATURE\", \"0\"))\n",
        "\n",
        "SCHEMA_CONFIG = {\n",
        "    \"node_types\": [\"Person\", \"Organization\", \"Location\", \"Event\", \"Document\"],\n",
        "    \"relationship_types\": [\n",
        "        \"MENTIONS\",\n",
        "        \"ASSOCIATED_WITH\",\n",
        "        \"LOCATED_IN\",\n",
        "        \"RELATED_TO\",\n",
        "        \"DESCRIBES\",\n",
        "    ],\n",
        "    \"patterns\": [\n",
        "        (\"Person\", \"ASSOCIATED_WITH\", \"Organization\"),\n",
        "        (\"Organization\", \"LOCATED_IN\", \"Location\"),\n",
        "        (\"Event\", \"RELATED_TO\", \"Organization\"),\n",
        "        (\"Document\", \"DESCRIBES\", \"Event\"),\n",
        "        (\"Document\", \"MENTIONS\", \"Person\"),\n",
        "    ],\n",
        "}\n",
        "\n",
        "llm = OpenAILLM(\n",
        "    model_name=LLM_MODEL,\n",
        "    model_params={\n",
        "        \"max_tokens\": LLM_MAX_TOKENS,\n",
        "        \"temperature\": LLM_TEMPERATURE,\n",
        "    },\n",
        ")\n",
        "embedder = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "kg_pipeline = SimpleKGPipeline(\n",
        "    llm=llm,\n",
        "    driver=driver,\n",
        "    embedder=embedder,\n",
        "    schema=SCHEMA_CONFIG,\n",
        "    from_pdf=False,\n",
        "    on_error=\"IGNORE\",\n",
        "    perform_entity_resolution=True,\n",
        "    neo4j_database=NEO4J_DATABASE,\n",
        ")\n",
        "\n",
        "print(\"Pipeline ready (LLM=%s, embedding=%s).\" % (LLM_MODEL, EMBED_MODEL))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run KG Builder on Markdown Sources\n",
        "\n",
        "Point the pipeline at your markdown drop. Each file is chunked, embedded, and merged into Neo4j automatically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except ImportError:\n",
        "    tqdm = None\n",
        "\n",
        "SOURCE_DIR = (PROJECT_ROOT / \"KnowledgeGraph\" / \"source_data\").resolve()\n",
        "if not SOURCE_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Missing source directory: {SOURCE_DIR}\")\n",
        "\n",
        "LIMIT_FILES = int(os.getenv(\"PIPELINE_FILE_LIMIT\", \"0\")) or None\n",
        "RUN_PIPELINE = False  # flip to True to start ingestion\n",
        "\n",
        "md_files: list[Path] = sorted(SOURCE_DIR.rglob(\"*.md\"))\n",
        "if LIMIT_FILES:\n",
        "    md_files = md_files[:LIMIT_FILES]\n",
        "\n",
        "print(f\"Found {len(md_files)} markdown files under {SOURCE_DIR}.\")\n",
        "\n",
        "async def _process_files(files: Iterable[Path]):\n",
        "    iterator = files\n",
        "    if tqdm is not None:\n",
        "        iterator = tqdm(files, desc=\"Ingesting\", unit=\"file\")\n",
        "    for path in iterator:\n",
        "        text = path.read_text(encoding=\"utf-8\")\n",
        "        metadata = {\"source_path\": str(path)}\n",
        "        await kg_pipeline.run_async(\n",
        "            file_path=str(path),\n",
        "            text=text,\n",
        "            document_metadata=metadata,\n",
        "        )\n",
        "\n",
        "if RUN_PIPELINE:\n",
        "    asyncio.run(_process_files(md_files))\n",
        "    print(\"Ingestion finished.\")\n",
        "else:\n",
        "    print(\"Pipeline not executed. Set RUN_PIPELINE = True to run.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Index Setup\n",
        "\n",
        "The pipeline stores chunk embeddings on `Chunk.embedding`. Use the official helpers to manage the vector index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j_graphrag.indexes import (\n",
        "    create_vector_index,\n",
        "    drop_index_if_exists,\n",
        "    retrieve_vector_index_info,\n",
        ")\n",
        "\n",
        "VECTOR_INDEX_NAME = os.getenv(\"NEO4J_VECTOR_INDEX\", \"chunk_embedding_idx\")\n",
        "VECTOR_LABEL = \"Chunk\"\n",
        "VECTOR_PROPERTY = \"embedding\"\n",
        "EMBED_DIM = int(os.getenv(\"OPENAI_EMBED_DIM\", \"1536\"))\n",
        "SIMILARITY_FN = os.getenv(\"NEO4J_SIMILARITY_FN\", \"cosine\").lower()\n",
        "\n",
        "if RUN_PIPELINE:\n",
        "    drop_index_if_exists(driver, VECTOR_INDEX_NAME, neo4j_database=NEO4J_DATABASE)\n",
        "    create_vector_index(\n",
        "        driver=driver,\n",
        "        name=VECTOR_INDEX_NAME,\n",
        "        label=VECTOR_LABEL,\n",
        "        embedding_property=VECTOR_PROPERTY,\n",
        "        dimensions=EMBED_DIM,\n",
        "        similarity_fn=SIMILARITY_FN,\n",
        "        neo4j_database=NEO4J_DATABASE,\n",
        "    )\n",
        "\n",
        "index_info = retrieve_vector_index_info(\n",
        "    driver=driver,\n",
        "    index_name=VECTOR_INDEX_NAME,\n",
        "    label_or_type=VECTOR_LABEL,\n",
        "    embedding_property=VECTOR_PROPERTY,\n",
        "    neo4j_database=NEO4J_DATABASE,\n",
        ")\n",
        "print(\"Index info:\", index_info)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Neo4j Load\n",
        "\n",
        "Run quick diagnostics to confirm that documents, chunks, and embeddings landed as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from textwrap import dedent\n",
        "\n",
        "\n",
        "def _single_value(query: str, key: str = \"c\"):\n",
        "    with driver.session(database=NEO4J_DATABASE) as session:\n",
        "        record = session.run(query).single()\n",
        "        return record[key] if record else 0\n",
        "\n",
        "\n",
        "def _list_records(query: str, limit: int = 5):\n",
        "    with driver.session(database=NEO4J_DATABASE) as session:\n",
        "        return session.run(query, limit=limit).data()\n",
        "\n",
        "stats = {\n",
        "    \"documents\": _single_value(\"MATCH (d:Document) RETURN count(d) AS c\"),\n",
        "    \"chunks\": _single_value(\"MATCH (c:Chunk) RETURN count(c) AS c\"),\n",
        "    \"entities\": _single_value(\"MATCH (e:__Entity__) RETURN count(e) AS c\"),\n",
        "}\n",
        "print(\"Counts:\", stats)\n",
        "\n",
        "sample_embeddings = _list_records(\n",
        "    \"MATCH (c:Chunk) WHERE c.embedding IS NOT NULL RETURN elementId(c) AS id, size(c.embedding) AS dim LIMIT $limit\"\n",
        ")\n",
        "print(\"Embedding sample:\", sample_embeddings[:3])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: End-to-End Question Answering\n",
        "\n",
        "Instantiate Neo4j's `GraphRAG` helper to verify retrieval quality against the freshly populated index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neo4j_graphrag.generation import GraphRAG\n",
        "from neo4j_graphrag.retrievers import VectorRetriever\n",
        "\n",
        "RUN_RAG_CHECK = False\n",
        "TEST_QUESTION = \"What recent announcements involve tenders or awards?\"\n",
        "\n",
        "if RUN_RAG_CHECK:\n",
        "    qa_llm = OpenAILLM(\n",
        "        model_name=LLM_MODEL,\n",
        "        model_params={\n",
        "            \"max_tokens\": LLM_MAX_TOKENS,\n",
        "            \"temperature\": 0,\n",
        "        },\n",
        "    )\n",
        "    qa_embedder = OpenAIEmbeddings(model=EMBED_MODEL)\n",
        "    retriever = VectorRetriever(\n",
        "        driver=driver,\n",
        "        index_name=VECTOR_INDEX_NAME,\n",
        "        embedder=qa_embedder,\n",
        "        neo4j_database=NEO4J_DATABASE,\n",
        "    )\n",
        "    graph_rag = GraphRAG(retriever=retriever, llm=qa_llm)\n",
        "    rag_result = graph_rag.search(\n",
        "        query_text=TEST_QUESTION,\n",
        "        retriever_config={\"top_k\": 5},\n",
        "        return_context=False,\n",
        "    )\n",
        "    print(rag_result.answer)\n",
        "else:\n",
        "    print(\"Set RUN_RAG_CHECK = True to issue a sample question.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "- Update `RUN_PIPELINE` and `RUN_RAG_CHECK` flags before execution.\n",
        "- Tune `SCHEMA_CONFIG` to match the entities and relationships you expect the model to extract.\n",
        "- Adjust model names or environment variables to align with your OpenAI deployment.\n",
        "- Once satisfied, consider checking the generated graph in Neo4j Browser (`CALL db.schema.visualization()`) or running ad-hoc Cypher validations.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
